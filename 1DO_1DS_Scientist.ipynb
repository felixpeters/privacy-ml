{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc5b3063-2ad7-41f5-a7b1-20ae5f46d7e5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4a458-592c-4655-aad9-361965349dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import torch\n",
    "from tools import models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "sy.load('opacus')\n",
    "np.random.seed(42) # The meaning of life!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb264f28-e816-4b03-a577-1ae908875347",
   "metadata": {},
   "source": [
    "### Join duet session (DS side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30127c81-74c2-4c22-aed8-e01b786a64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "duet = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64697fdf-44d0-4751-8345-0563fd67c139",
   "metadata": {},
   "source": [
    "### Getting pointers to data in duet store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e651e4cb-754b-4498-92c5-de1b8ec55fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the pionters to the data\n",
    "time.sleep(31) # Sleep timer so you can just press restart on both notebooks without caring about loading time (might need to be adjusted)\n",
    "\n",
    "train_data_ptr = duet.store[0]\n",
    "train_labels_ptr = duet.store[1]\n",
    "\n",
    "test_data_ptr = duet.store[2]\n",
    "test_labels_ptr = duet.store[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0703973-0052-4a4d-989b-46d3fdf0b2d8",
   "metadata": {},
   "source": [
    "### Parameters and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f48c5a5-59fd-4797-97e1-8f8dd4795c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for tracking purposes\n",
    "MODEL = 'Deep2DNet'\n",
    "DATASET = 'MedNIST'\n",
    "TRACKING = False # Whether or not this run should be tracked in the results csv file\n",
    "DP = True # Whether or not Differential Privacy should be applied\n",
    "\n",
    "# Parameters for training and Differential Privacy\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 18\n",
    "LEARNING_RATE = 0.0025 if DP else 0.002\n",
    "\n",
    "DELTA = 1e-4 # Set to be less then the inverse of the size of the training dataset (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "NOISE_MULTIPLIER = 2.0 # The amount of noise sampled and added to the average of the gradients in a batch (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "MAX_GRAD_NORM = 0.1 # The maximum L2 norm of per-sample gradients before they are aggregated by the averaging step (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "\n",
    "length = len(train_data_ptr)\n",
    "SAMPLE_SIZE = length - length % BATCH_SIZE # NOTE: Current implementation only trains data in multiples of batch size. So BATCH_SIZE % LENGTH amount of data will not be used for training.\n",
    "SAMPLE_RATE = BATCH_SIZE / SAMPLE_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b0b10-7b4e-4280-b3aa-ce71ed689f8f",
   "metadata": {},
   "source": [
    "### Preparation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d603d-da4d-4650-a06c-69411dc72724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting remote and local instances\n",
    "local_model = models.Deep2DNet(torch)\n",
    "remote_model = local_model.send(duet)\n",
    "remote_torch = duet.torch\n",
    "remote_opacus = duet.opacus\n",
    "\n",
    "# Setting device to train on\n",
    "cuda_available = remote_torch.cuda.is_available().get(request_block=True, reason='Need to check for available GPU!')\n",
    "if cuda_available:\n",
    "    device = remote_torch.device('cuda:0')\n",
    "    remote_model.cuda(device)\n",
    "else:\n",
    "    device = remote_torch.device('cpu')\n",
    "    remote_model.cpu()\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "params = remote_model.parameters()\n",
    "optim = remote_torch.optim.Adam(params=params, lr=LEARNING_RATE)\n",
    "loss_function = remote_torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Setting up Differential Privacy Engine\n",
    "if DP:\n",
    "    privacy_engine_ptr = remote_opacus.privacy_engine.PrivacyEngine(\n",
    "        remote_model.real_module, sample_rate=SAMPLE_RATE,\n",
    "        noise_multiplier=NOISE_MULTIPLIER, max_grad_norm=MAX_GRAD_NORM\n",
    "    )\n",
    "    privacy_engine_ptr.to(device)\n",
    "    privacy_engine_ptr.attach(optim)\n",
    "else:\n",
    "    privacy_engine_ptr = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1761ad-d4af-4dae-a221-41077f678119",
   "metadata": {},
   "source": [
    "### Training and testing the remote model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983458aa-a990-4722-aba4-9ca3fc9abc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.utils import train, test\n",
    "\n",
    "losses, test_accs, test_losses, epsilons, alphas, epoch_times = train(BATCH_SIZE, EPOCHS, DELTA, \n",
    "                                                                      remote_model, remote_torch,\n",
    "                                                                      optim, loss_function, \n",
    "                                                                      train_data_ptr, train_labels_ptr, \n",
    "                                                                      test_data_ptr, test_labels_ptr, \n",
    "                                                                      [1, 64, 64], device, privacy_engine_ptr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b517e4-b6df-471e-a5c5-ce0e27e936b5",
   "metadata": {},
   "source": [
    "### Validating the resulting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9521ae8-ba3b-4697-bc11-4247f07d2b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import datasets\n",
    "# TODO!: Use best model for validation\n",
    "# Sadly very redundant since remotly tracking best model is not easily possible and therefore just last model is used for validation\n",
    "# Evalutating the model locally with the validation data\n",
    "eval_model = remote_model.get(request_block=True, reason=\"Needed for local evaluation!\")\n",
    "eval_model.cuda(torch.device('cuda:0'))\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Higher sample_size here as on DO side will ensure that it is actually data that is not in train or test set!\n",
    "_, _, val_ds = datasets.Loader.load_MedNIST(sample_size=0.1, test_size=0.1, val_size=0.05)\n",
    "val_data, val_labels = val_ds.as_tensor()\n",
    "print(len(val_data))\n",
    "\n",
    "val_acc, val_loss = test(eval_model, loss_function, torch, val_data, val_labels, torch.device('cuda:0'))\n",
    "\n",
    "print(f'Validation Accuracy: {val_acc} ---- Validation Loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfd599e-9503-4ca6-966c-f5c402cd616b",
   "metadata": {},
   "source": [
    "### Tracking all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f07ae45-0590-41e0-a203-1e3c1151307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking all interesting variables and results in .csv file\n",
    "if TRACKING:\n",
    "    d = {\n",
    "        'model': MODEL,\n",
    "        'dataset': DATASET,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epochs': EPOCHS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'train_sample_size': SAMPLE_SIZE,\n",
    "        'test_sample_size': len(test_data_ptr),\n",
    "        'val_sample_size': len(val_data),\n",
    "        'delta': DELTA,\n",
    "        'noise_multiplier': NOISE_MULTIPLIER,\n",
    "        'max_grad_norm': MAX_GRAD_NORM,\n",
    "        'dp_used': DP,\n",
    "        'epsilons': epsilons,\n",
    "        'alphas': alphas,\n",
    "        'train_losses': losses,\n",
    "        'test_accs': test_accs,\n",
    "        'test_losses': test_losses,\n",
    "        'val_acc': val_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'epoch_times': epoch_times\n",
    "    }      \n",
    "    df = pd.read_csv('./Results/1DO-1DS.csv')\n",
    "    df = df.append(d, ignore_index=True)\n",
    "    df.to_csv('./Results/1DO-1DS.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
