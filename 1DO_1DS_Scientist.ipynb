{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b4a458-592c-4655-aad9-361965349dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import torch\n",
    "from tools import models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "sy.load('opacus')\n",
    "np.random.seed(42) # The meaning of life!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30127c81-74c2-4c22-aed8-e01b786a64e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "unconfined": true,
       "width": 400
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎤  🎸  ♪♪♪ Joining Duet ♫♫♫  🎻  🎹\n",
      "\n",
      "♫♫♫ >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "♫♫♫ > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > ❤️ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "♫♫♫ > Punching through firewall to OpenGrid Network Node at:\n",
      "♫♫♫ > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "♫♫♫ >\n",
      "♫♫♫ > ...waiting for response from OpenGrid Network... \n",
      "♫♫♫ > \u001b[92mDONE!\u001b[0m\n",
      "\n",
      "♫♫♫ > \u001b[92mCONNECTED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "duet = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e651e4cb-754b-4498-92c5-de1b8ec55fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the pionters to the data\n",
    "time.sleep(31) # Sleep timer so you can just press restart on both notebooks without caring (might need to be adjusted)\n",
    "\n",
    "train_data_ptr = duet.store[0]\n",
    "train_labels_ptr = duet.store[1]\n",
    "\n",
    "test_data_ptr = duet.store[2]\n",
    "test_labels_ptr = duet.store[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "315d603d-da4d-4650-a06c-69411dc72724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for tracking purposes\n",
    "MODEL = 'Deep2DNet'\n",
    "DATASET = 'MedNIST'\n",
    "TRACKING = True # Whether or not this run should be tracked in the results csv file\n",
    "DP = False # Whether or not Differential Privacy should be applied\n",
    "\n",
    "# Parameters for training and Differential Privacy\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.003 if DP else 0.001\n",
    "\n",
    "DELTA = 1e-4 # Set to be less then the inverse of the size of the training dataset (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "NOISE_MULTIPLIER = 2.0 # The amount of noise sampled and added to the average of the gradients in a batch (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "MAX_GRAD_NORM = 1.2 # The maximum L2 norm of per-sample gradients before they are aggregated by the averaging step (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "\n",
    "length = len(train_data_ptr)\n",
    "SAMPLE_SIZE = length - length % BATCH_SIZE # NOTE: Current implementation only trains data in multiples of batch size. So BATCH_SIZE % LENGTH amount of data will not be used for training.\n",
    "SAMPLE_RATE = BATCH_SIZE / SAMPLE_SIZE\n",
    "\n",
    "# Getting remote and local instances\n",
    "local_model = models.Deep2DNet(torch)\n",
    "remote_model = local_model.send(duet)\n",
    "remote_torch = duet.torch\n",
    "remote_opacus = duet.opacus\n",
    "\n",
    "# Setting device to train on\n",
    "cuda_available = remote_torch.cuda.is_available().get(request_block=True, reason='Need to check for available GPU!')\n",
    "if cuda_available:\n",
    "    device = remote_torch.device('cuda:0')\n",
    "    remote_model.cuda(device)\n",
    "else:\n",
    "    device = remote_torch.device('cpu')\n",
    "    remote_model.cpu()\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "params = remote_model.parameters()\n",
    "optim = remote_torch.optim.Adam(params=params, lr=LEARNING_RATE) # without DP: 0.0001 // with DP: 0.002-0.003\n",
    "loss_function = remote_torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Setting up Differential Privacy Engine\n",
    "if DP:\n",
    "    privacy_engine_ptr = remote_opacus.privacy_engine.PrivacyEngine(\n",
    "        remote_model.real_module, sample_rate=SAMPLE_RATE,\n",
    "        noise_multiplier=NOISE_MULTIPLIER, max_grad_norm=MAX_GRAD_NORM\n",
    "    )\n",
    "    privacy_engine_ptr.attach(optim)\n",
    "else:\n",
    "    privacy_engine_ptr = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7519d7d2-7dbd-4a04-aab8-8ae0a68dfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(batch_size, epochs, model, \n",
    "          torch_ref, optim, loss_function, \n",
    "          train_data, train_labels, test_data, \n",
    "          test_labels, input_shape, device, privacy_engine=None):\n",
    "    \n",
    "    # Variables to track\n",
    "    losses = [] # Training losses per batch per epoch\n",
    "    test_accs = []\n",
    "    test_losses = [] # Test losses per epoch\n",
    "    epsilons = [] \n",
    "    alphas = []\n",
    "    epoch_times = [] # Training times for each epoch\n",
    "    best_acc_loss = (0, 0)\n",
    "    best_model = None\n",
    "    \n",
    "    # Divide dataset into batches (sadly remote DataLoaders aren't yet a thing in pysyft)\n",
    "    length = len(train_data)\n",
    "    \n",
    "    if length % batch_size != 0:\n",
    "        cut_data = train_data[:length - length % batch_size]\n",
    "        cut_labels = train_labels[:length - length % batch_size]\n",
    "        \n",
    "    shape = [-1, batch_size]\n",
    "    shape.extend(input_shape)\n",
    "    \n",
    "    batch_data = cut_data.view(shape)\n",
    "    batch_labels = cut_labels.view(-1, batch_size)\n",
    "    \n",
    "    # Prepare indices for randomization of order for each epoch\n",
    "    indices = np.arange(int(length / batch_size))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        epoch_loss = []\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        print(f'###### Epoch {epoch + 1} ######')\n",
    "        for i in indices:\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            output = model(batch_data[int(i)].to(device))\n",
    "            \n",
    "            loss = loss_function(output, batch_labels[int(i)].to(device))\n",
    "            loss_item = loss.item()\n",
    "            \n",
    "            if model.is_local:\n",
    "                loss_value = loss_item\n",
    "            else:\n",
    "                loss_value = loss_item.get(reason=\"To evaluate training progress\", request_block=True, timeout_secs=5)\n",
    "            print(f'Training Loss: {loss_value}')\n",
    "            epoch_loss.append(loss_value)\n",
    "        \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        # Checking our privacy budget\n",
    "        if privacy_engine is not None:\n",
    "            epsilon_tuple = privacy_engine.get_privacy_spent(DELTA)\n",
    "            epsilon_ptr = epsilon_tuple[0].resolve_pointer_type()\n",
    "            best_alpha_ptr = epsilon_tuple[1].resolve_pointer_type()\n",
    "\n",
    "            epsilon = epsilon_ptr.get(\n",
    "                reason=\"So we dont go over it\",\n",
    "                request_block=True,\n",
    "                timeout_secs=5\n",
    "            )\n",
    "            best_alpha = best_alpha_ptr.get(\n",
    "                reason=\"So we dont go over it\",\n",
    "                request_block=True,\n",
    "                timeout_secs=5\n",
    "            )\n",
    "            if epsilon is None:\n",
    "                epsilon = float(\"-inf\")\n",
    "            if best_alpha is None:\n",
    "                best_alpha = float(\"-inf\")\n",
    "            print(\n",
    "                f\"(ε = {epsilon:.2f}, δ = {DELTA}) for α = {best_alpha}\"\n",
    "            )\n",
    "            epsilons.append(epsilon)\n",
    "            alphas.append(best_alpha)\n",
    "    \n",
    "        test_acc, test_loss = test(model, loss_function, torch_ref, test_data, test_labels, device)\n",
    "        print(f'Test Accuracy: {test_acc} ---- Test Loss: {test_loss}')\n",
    "        \n",
    "        epoch_end = time.time()\n",
    "        print(f\"Epoch time: {float(epoch_end - epoch_start)} seconds\")\n",
    "        \n",
    "        losses.append(epoch_loss)\n",
    "        epoch_times.append(float(epoch_end - epoch_start))\n",
    "        test_accs.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "    return losses, test_accs, test_losses, epsilons, alphas, epoch_times\n",
    "                   \n",
    "            \n",
    "def test(model, loss_function, torch_ref, data, labels, device):\n",
    "    model.eval()\n",
    "    \n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    length = len(data)\n",
    "    \n",
    "    with torch_ref.no_grad():\n",
    "        output = model(data)\n",
    "        test_loss = loss_function(output, labels)\n",
    "        prediction = output.argmax(dim=1)\n",
    "        total = prediction.eq(labels).sum().item()\n",
    "        \n",
    "    acc_ptr = total / length\n",
    "    if model.is_local:\n",
    "        acc = acc_ptr\n",
    "        loss = test_loss.item()\n",
    "    else:\n",
    "        acc = acc_ptr.get(reason=\"To evaluate training progress\", request_block=True, timeout_secs=5)\n",
    "        loss = test_loss.item().get(reason=\"To evaluate training progress\", request_block=True, timeout_secs=5)\n",
    "\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "983458aa-a990-4722-aba4-9ca3fc9abc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Epoch 1 ######\n",
      "Training Loss: 1.7919745445251465\n",
      "Training Loss: 1.7790114879608154\n",
      "Training Loss: 2.3523731231689453\n",
      "Training Loss: 1.7817648649215698\n",
      "Training Loss: 1.7911075353622437\n",
      "Training Loss: 1.7917263507843018\n",
      "Training Loss: 1.796738624572754\n",
      "Training Loss: 1.78407621383667\n",
      "Training Loss: 1.7510606050491333\n",
      "Training Loss: 1.699795126914978\n",
      "Training Loss: 1.6799260377883911\n",
      "Training Loss: 1.6558696031570435\n",
      "Training Loss: 1.6099357604980469\n",
      "Training Loss: 1.5818684101104736\n",
      "Training Loss: 1.5260014533996582\n",
      "Training Loss: 1.4664521217346191\n",
      "Training Loss: 1.3777302503585815\n",
      "Training Loss: 1.7096197605133057\n",
      "Test Accuracy: 0.42718446254730225 ---- Test Loss: 1.3412050008773804\n",
      "Epoch time: 6.171245813369751 seconds\n",
      "###### Epoch 2 ######\n",
      "Training Loss: 1.3024133443832397\n",
      "Training Loss: 1.096971869468689\n",
      "Training Loss: 1.11940336227417\n",
      "Training Loss: 1.0986227989196777\n",
      "Training Loss: 1.0980160236358643\n",
      "Training Loss: 1.1676851511001587\n",
      "Training Loss: 1.0450563430786133\n",
      "Training Loss: 1.0259488821029663\n",
      "Training Loss: 1.0369672775268555\n",
      "Training Loss: 0.9855812788009644\n",
      "Training Loss: 1.0093857049942017\n",
      "Training Loss: 0.9107543230056763\n",
      "Training Loss: 0.8033967018127441\n",
      "Training Loss: 1.0969874858856201\n",
      "Training Loss: 0.9021146893501282\n",
      "Training Loss: 0.7149143815040588\n",
      "Training Loss: 1.1183222532272339\n",
      "Training Loss: 1.128788709640503\n",
      "Test Accuracy: 0.5388349294662476 ---- Test Loss: 0.9838762879371643\n",
      "Epoch time: 5.835819721221924 seconds\n",
      "###### Epoch 3 ######\n",
      "Training Loss: 1.0088069438934326\n",
      "Training Loss: 0.9804171919822693\n",
      "Training Loss: 0.9665499925613403\n",
      "Training Loss: 1.0053105354309082\n",
      "Training Loss: 0.9330611228942871\n",
      "Training Loss: 0.8713802099227905\n",
      "Training Loss: 0.8671496510505676\n",
      "Training Loss: 0.7474891543388367\n",
      "Training Loss: 0.9930484294891357\n",
      "Training Loss: 0.8078206777572632\n",
      "Training Loss: 0.8800302743911743\n",
      "Training Loss: 0.7086809277534485\n",
      "Training Loss: 0.7078708410263062\n",
      "Training Loss: 1.478570818901062\n",
      "Training Loss: 1.1219717264175415\n",
      "Training Loss: 0.7027018070220947\n",
      "Training Loss: 0.6921395063400269\n",
      "Training Loss: 0.8260143995285034\n",
      "Test Accuracy: 0.6456310749053955 ---- Test Loss: 0.7981628179550171\n",
      "Epoch time: 6.778750896453857 seconds\n",
      "###### Epoch 4 ######\n",
      "Training Loss: 0.6902004480361938\n",
      "Training Loss: 0.844173789024353\n",
      "Training Loss: 0.8001589179039001\n",
      "Training Loss: 0.5872650742530823\n",
      "Training Loss: 0.8406736850738525\n",
      "Training Loss: 0.6953747272491455\n",
      "Training Loss: 0.5657470226287842\n",
      "Training Loss: 0.5520535111427307\n",
      "Training Loss: 0.7935798764228821\n",
      "Training Loss: 0.535815417766571\n",
      "Training Loss: 0.7006633877754211\n",
      "Training Loss: 0.6488307118415833\n",
      "Training Loss: 0.9003576636314392\n",
      "Training Loss: 0.680056631565094\n",
      "Training Loss: 0.5136523246765137\n",
      "Training Loss: 0.44621509313583374\n",
      "Training Loss: 0.6774186491966248\n",
      "Training Loss: 0.592197597026825\n",
      "Test Accuracy: 0.8737863898277283 ---- Test Loss: 0.4801779091358185\n",
      "Epoch time: 4.938599109649658 seconds\n",
      "###### Epoch 5 ######\n",
      "Training Loss: 0.674864649772644\n",
      "Training Loss: 0.5092491507530212\n",
      "Training Loss: 0.4737294912338257\n",
      "Training Loss: 0.2670048773288727\n",
      "Training Loss: 0.23914246261119843\n",
      "Training Loss: 0.38720080256462097\n",
      "Training Loss: 0.3744097054004669\n",
      "Training Loss: 0.2756836712360382\n",
      "Training Loss: 0.6429225206375122\n",
      "Training Loss: 0.5353950262069702\n",
      "Training Loss: 0.2800966203212738\n",
      "Training Loss: 0.2656962275505066\n",
      "Training Loss: 0.17721667885780334\n",
      "Training Loss: 0.25139495730400085\n",
      "Training Loss: 0.37887269258499146\n",
      "Training Loss: 0.22238518297672272\n",
      "Training Loss: 0.28514236211776733\n",
      "Training Loss: 0.20830149948596954\n",
      "Test Accuracy: 0.8980582356452942 ---- Test Loss: 0.3095840811729431\n",
      "Epoch time: 5.884702920913696 seconds\n",
      "###### Epoch 6 ######\n",
      "Training Loss: 0.17907913029193878\n",
      "Training Loss: 0.2980726957321167\n",
      "Training Loss: 0.1431570202112198\n",
      "Training Loss: 0.1471814662218094\n",
      "Training Loss: 0.21165631711483002\n",
      "Training Loss: 0.290995717048645\n",
      "Training Loss: 0.1873365044593811\n",
      "Training Loss: 0.18086516857147217\n",
      "Training Loss: 0.200436532497406\n",
      "Training Loss: 0.08656495809555054\n",
      "Training Loss: 0.1790207326412201\n",
      "Training Loss: 0.27636468410491943\n",
      "Training Loss: 0.11073371022939682\n",
      "Training Loss: 0.061539843678474426\n",
      "Training Loss: 0.23297365009784698\n",
      "Training Loss: 0.18116521835327148\n",
      "Training Loss: 0.08387758582830429\n",
      "Training Loss: 0.11222834885120392\n",
      "Test Accuracy: 0.9660193920135498 ---- Test Loss: 0.1233525425195694\n",
      "Epoch time: 6.812096357345581 seconds\n",
      "###### Epoch 7 ######\n",
      "Training Loss: 0.13589932024478912\n",
      "Training Loss: 0.12363988161087036\n",
      "Training Loss: 0.11187594383955002\n",
      "Training Loss: 0.22158783674240112\n",
      "Training Loss: 0.08723971992731094\n",
      "Training Loss: 0.15444529056549072\n",
      "Training Loss: 0.15109612047672272\n",
      "Training Loss: 0.08187554031610489\n",
      "Training Loss: 0.2810080945491791\n",
      "Training Loss: 0.32438480854034424\n",
      "Training Loss: 0.08804028481245041\n",
      "Training Loss: 0.1963718980550766\n",
      "Training Loss: 0.18790572881698608\n",
      "Training Loss: 0.1947249174118042\n",
      "Training Loss: 0.1911366879940033\n",
      "Training Loss: 0.07515087723731995\n",
      "Training Loss: 0.17030751705169678\n",
      "Training Loss: 0.10291633009910583\n",
      "Test Accuracy: 0.9271844625473022 ---- Test Loss: 0.1808241754770279\n",
      "Epoch time: 5.844558477401733 seconds\n",
      "###### Epoch 8 ######\n",
      "Training Loss: 0.12669801712036133\n",
      "Training Loss: 0.17994588613510132\n",
      "Training Loss: 0.16769447922706604\n",
      "Training Loss: 0.25342893600463867\n",
      "Training Loss: 0.05808108672499657\n",
      "Training Loss: 0.07329873740673065\n",
      "Training Loss: 0.09466641396284103\n",
      "Training Loss: 0.0930275246500969\n",
      "Training Loss: 0.09758258610963821\n",
      "Training Loss: 0.13142676651477814\n",
      "Training Loss: 0.08332299441099167\n",
      "Training Loss: 0.07470236718654633\n",
      "Training Loss: 0.13793770968914032\n",
      "Training Loss: 0.04848423972725868\n",
      "Training Loss: 0.06072204187512398\n",
      "Training Loss: 0.13798533380031586\n",
      "Training Loss: 0.17378023266792297\n",
      "Training Loss: 0.06287069618701935\n",
      "Test Accuracy: 0.9660193920135498 ---- Test Loss: 0.08730919659137726\n",
      "Epoch time: 5.836271524429321 seconds\n",
      "###### Epoch 9 ######\n",
      "Training Loss: 0.08217255771160126\n",
      "Training Loss: 0.045443154871463776\n",
      "Training Loss: 0.06086308881640434\n",
      "Training Loss: 0.17614740133285522\n",
      "Training Loss: 0.04097361862659454\n",
      "Training Loss: 0.048042379319667816\n",
      "Training Loss: 0.06864918023347855\n",
      "Training Loss: 0.0974002256989479\n",
      "Training Loss: 0.07873234897851944\n",
      "Training Loss: 0.08301877230405807\n",
      "Training Loss: 0.06819315254688263\n",
      "Training Loss: 0.033324580639600754\n",
      "Training Loss: 0.02121030166745186\n",
      "Training Loss: 0.1471731960773468\n",
      "Training Loss: 0.04315799102187157\n",
      "Training Loss: 0.05165354907512665\n",
      "Training Loss: 0.037039175629615784\n",
      "Training Loss: 0.06280079483985901\n",
      "Test Accuracy: 0.9660193920135498 ---- Test Loss: 0.0952899232506752\n",
      "Epoch time: 5.84297251701355 seconds\n",
      "###### Epoch 10 ######\n",
      "Training Loss: 0.0156985055655241\n",
      "Training Loss: 0.03111237846314907\n",
      "Training Loss: 0.1868678331375122\n",
      "Training Loss: 0.04263683408498764\n",
      "Training Loss: 0.029062485322356224\n",
      "Training Loss: 0.05916009843349457\n",
      "Training Loss: 0.07289864122867584\n",
      "Training Loss: 0.07633424550294876\n",
      "Training Loss: 0.02624235861003399\n",
      "Training Loss: 0.05920784920454025\n",
      "Training Loss: 0.07257276028394699\n",
      "Training Loss: 0.05821029096841812\n",
      "Training Loss: 0.028119206428527832\n",
      "Training Loss: 0.176054909825325\n",
      "Training Loss: 0.0790799930691719\n",
      "Training Loss: 0.051129404455423355\n",
      "Training Loss: 0.0651908665895462\n",
      "Training Loss: 0.04759112372994423\n",
      "Test Accuracy: 0.9660193920135498 ---- Test Loss: 0.1200992539525032\n",
      "Epoch time: 4.91350531578064 seconds\n",
      "###### Epoch 11 ######\n",
      "Training Loss: 0.04988305643200874\n",
      "Training Loss: 0.09351394325494766\n",
      "Training Loss: 0.055949315428733826\n",
      "Training Loss: 0.0523056760430336\n",
      "Training Loss: 0.07037204504013062\n",
      "Training Loss: 0.1223696693778038\n",
      "Training Loss: 0.020415637642145157\n",
      "Training Loss: 0.0900200754404068\n",
      "Training Loss: 0.02293057180941105\n",
      "Training Loss: 0.031021030619740486\n",
      "Training Loss: 0.04402931034564972\n",
      "Training Loss: 0.04836441949009895\n",
      "Training Loss: 0.06457268446683884\n",
      "Training Loss: 0.10658809542655945\n",
      "Training Loss: 0.034879542887210846\n",
      "Training Loss: 0.06704208254814148\n",
      "Training Loss: 0.020906396210193634\n",
      "Training Loss: 0.032098256051540375\n",
      "Test Accuracy: 0.9854369163513184 ---- Test Loss: 0.048093389719724655\n",
      "Epoch time: 7.788187742233276 seconds\n",
      "###### Epoch 12 ######\n",
      "Training Loss: 0.028315452858805656\n",
      "Training Loss: 0.04890356957912445\n",
      "Training Loss: 0.027131643146276474\n",
      "Training Loss: 0.04961448162794113\n",
      "Training Loss: 0.023753030225634575\n",
      "Training Loss: 0.03185788169503212\n",
      "Training Loss: 0.05472809821367264\n",
      "Training Loss: 0.041857801377773285\n",
      "Training Loss: 0.011133082211017609\n",
      "Training Loss: 0.039977822452783585\n",
      "Training Loss: 0.01645701751112938\n",
      "Training Loss: 0.007041436620056629\n",
      "Training Loss: 0.0929841622710228\n",
      "Training Loss: 0.05502459034323692\n",
      "Training Loss: 0.03168680518865585\n",
      "Training Loss: 0.006883593276143074\n",
      "Training Loss: 0.023471690714359283\n",
      "Training Loss: 0.04913026839494705\n",
      "Test Accuracy: 0.9805825352668762 ---- Test Loss: 0.06209386885166168\n",
      "Epoch time: 4.954179763793945 seconds\n",
      "###### Epoch 13 ######\n",
      "Training Loss: 0.024111812934279442\n",
      "Training Loss: 0.07845170795917511\n",
      "Training Loss: 0.036243606358766556\n",
      "Training Loss: 0.02485140599310398\n",
      "Training Loss: 0.041957754641771317\n",
      "Training Loss: 0.029112033545970917\n",
      "Training Loss: 0.03750602528452873\n",
      "Training Loss: 0.03914603963494301\n",
      "Training Loss: 0.06679864227771759\n",
      "Training Loss: 0.010955505073070526\n",
      "Training Loss: 0.09459035098552704\n",
      "Training Loss: 0.09217026084661484\n",
      "Training Loss: 0.04763372987508774\n",
      "Training Loss: 0.09261049330234528\n",
      "Training Loss: 0.06213533505797386\n",
      "Training Loss: 0.12246502935886383\n",
      "Training Loss: 0.042342618107795715\n",
      "Training Loss: 0.024478638544678688\n",
      "Test Accuracy: 0.9563106894493103 ---- Test Loss: 0.1124139130115509\n",
      "Epoch time: 4.91461968421936 seconds\n",
      "###### Epoch 14 ######\n",
      "Training Loss: 0.021007142961025238\n",
      "Training Loss: 0.08564237505197525\n",
      "Training Loss: 0.08220455050468445\n",
      "Training Loss: 0.03945577144622803\n",
      "Training Loss: 0.0437549464404583\n",
      "Training Loss: 0.060785118490457535\n",
      "Training Loss: 0.02683304250240326\n",
      "Training Loss: 0.053100280463695526\n",
      "Training Loss: 0.03188806027173996\n",
      "Training Loss: 0.01955685392022133\n",
      "Training Loss: 0.014995740726590157\n",
      "Training Loss: 0.06647710502147675\n",
      "Training Loss: 0.05616787075996399\n",
      "Training Loss: 0.02214793860912323\n",
      "Training Loss: 0.024436144158244133\n",
      "Training Loss: 0.02303304895758629\n",
      "Training Loss: 0.03573021665215492\n",
      "Training Loss: 0.10991141200065613\n",
      "Test Accuracy: 0.9805825352668762 ---- Test Loss: 0.058953411877155304\n",
      "Epoch time: 5.829446792602539 seconds\n",
      "###### Epoch 15 ######\n",
      "Training Loss: 0.07006761431694031\n",
      "Training Loss: 0.03186890482902527\n",
      "Training Loss: 0.004252070561051369\n",
      "Training Loss: 0.03213993087410927\n",
      "Training Loss: 0.05350668728351593\n",
      "Training Loss: 0.06243588402867317\n",
      "Training Loss: 0.032178282737731934\n",
      "Training Loss: 0.08862295001745224\n",
      "Training Loss: 0.04638907313346863\n",
      "Training Loss: 0.012190976180136204\n",
      "Training Loss: 0.04792103171348572\n",
      "Training Loss: 0.01772305928170681\n",
      "Training Loss: 0.025345299392938614\n",
      "Training Loss: 0.09972533583641052\n",
      "Training Loss: 0.05563552305102348\n",
      "Training Loss: 0.0508907288312912\n",
      "Training Loss: 0.04059012234210968\n",
      "Training Loss: 0.08747779577970505\n",
      "Test Accuracy: 0.9854369163513184 ---- Test Loss: 0.1595534384250641\n",
      "Epoch time: 5.83513069152832 seconds\n",
      "###### Epoch 16 ######\n",
      "Training Loss: 0.027350181713700294\n",
      "Training Loss: 0.027437608689069748\n",
      "Training Loss: 0.028523731976747513\n",
      "Training Loss: 0.15840010344982147\n",
      "Training Loss: 0.026301203295588493\n",
      "Training Loss: 1.8192012310028076\n",
      "Training Loss: 0.015052799135446548\n",
      "Training Loss: 0.04166756942868233\n",
      "Training Loss: 0.21628360450267792\n",
      "Training Loss: 0.2630009353160858\n",
      "Training Loss: 0.6068220138549805\n",
      "Training Loss: 0.1815193146467209\n",
      "Training Loss: 0.22803039848804474\n",
      "Training Loss: 0.14087654650211334\n",
      "Training Loss: 0.20290474593639374\n",
      "Training Loss: 0.2858579754829407\n",
      "Training Loss: 0.1806780993938446\n",
      "Training Loss: 0.2005222886800766\n",
      "Test Accuracy: 0.9563106894493103 ---- Test Loss: 0.20782801508903503\n",
      "Epoch time: 5.852644443511963 seconds\n",
      "###### Epoch 17 ######\n",
      "Training Loss: 0.15649299323558807\n",
      "Training Loss: 0.15637564659118652\n",
      "Training Loss: 0.12278131395578384\n",
      "Training Loss: 0.06190008670091629\n",
      "Training Loss: 0.08996233344078064\n",
      "Training Loss: 0.06623069941997528\n",
      "Training Loss: 0.08264482766389847\n",
      "Training Loss: 0.04573808237910271\n",
      "Training Loss: 0.11220541596412659\n",
      "Training Loss: 0.045800257474184036\n",
      "Training Loss: 0.06096561253070831\n",
      "Training Loss: 0.02917879819869995\n",
      "Training Loss: 0.13701453804969788\n",
      "Training Loss: 0.048074282705783844\n",
      "Training Loss: 0.020005879923701286\n",
      "Training Loss: 0.05386873707175255\n",
      "Training Loss: 0.06973181664943695\n",
      "Training Loss: 0.07963590323925018\n",
      "Test Accuracy: 0.9757281541824341 ---- Test Loss: 0.051132310181856155\n",
      "Epoch time: 6.807955503463745 seconds\n",
      "###### Epoch 18 ######\n",
      "Training Loss: 0.03904823586344719\n",
      "Training Loss: 0.011712661944329739\n",
      "Training Loss: 0.029674220830202103\n",
      "Training Loss: 0.03450503200292587\n",
      "Training Loss: 0.030211208388209343\n",
      "Training Loss: 0.04714905843138695\n",
      "Training Loss: 0.012918262742459774\n",
      "Training Loss: 0.06186594441533089\n",
      "Training Loss: 0.0174642913043499\n",
      "Training Loss: 0.047323666512966156\n",
      "Training Loss: 0.032653164118528366\n",
      "Training Loss: 0.03613206744194031\n",
      "Training Loss: 0.0043057347647845745\n",
      "Training Loss: 0.08240607380867004\n",
      "Training Loss: 0.006560562644153833\n",
      "Training Loss: 0.009965122677385807\n",
      "Training Loss: 0.06786336749792099\n",
      "Training Loss: 0.012207726016640663\n",
      "Test Accuracy: 0.9854369163513184 ---- Test Loss: 0.11294658482074738\n",
      "Epoch time: 5.9022300243377686 seconds\n",
      "###### Epoch 19 ######\n",
      "Training Loss: 0.003585077589377761\n",
      "Training Loss: 0.03317845240235329\n",
      "Training Loss: 0.0062744515016674995\n",
      "Training Loss: 0.009207140654325485\n",
      "Training Loss: 0.03608444333076477\n",
      "Training Loss: 0.06627088785171509\n",
      "Training Loss: 0.021412570029497147\n",
      "Training Loss: 0.03150220215320587\n",
      "Training Loss: 0.022575989365577698\n",
      "Training Loss: 0.010013562627136707\n",
      "Training Loss: 0.04178568720817566\n",
      "Training Loss: 0.011258094571530819\n",
      "Training Loss: 0.0023207420017570257\n",
      "Training Loss: 0.054966144263744354\n",
      "Training Loss: 0.03342665731906891\n",
      "Training Loss: 0.2783433496952057\n",
      "Training Loss: 0.041557058691978455\n",
      "Training Loss: 0.018905682489275932\n",
      "Test Accuracy: 0.9708737730979919 ---- Test Loss: 0.08209510892629623\n",
      "Epoch time: 5.856134653091431 seconds\n",
      "###### Epoch 20 ######\n",
      "Training Loss: 0.09286393970251083\n",
      "Training Loss: 0.06390749663114548\n",
      "Training Loss: 0.019292427226901054\n",
      "Training Loss: 0.04828028380870819\n",
      "Training Loss: 0.040698494762182236\n",
      "Training Loss: 0.017345309257507324\n",
      "Training Loss: 0.08311887085437775\n",
      "Training Loss: 0.027969028800725937\n",
      "Training Loss: 0.013881959952414036\n",
      "Training Loss: 0.022021371871232986\n",
      "Training Loss: 0.06873475760221481\n",
      "Training Loss: 0.009791400283575058\n",
      "Training Loss: 0.055931586772203445\n",
      "Training Loss: 0.019610155373811722\n",
      "Training Loss: 0.0030399165116250515\n",
      "Training Loss: 0.8230665326118469\n",
      "Training Loss: 0.0046748085878789425\n",
      "Training Loss: 0.03223852440714836\n",
      "Test Accuracy: 0.9126213788986206 ---- Test Loss: 0.2044072151184082\n",
      "Epoch time: 6.809666156768799 seconds\n",
      "###### Epoch 21 ######\n",
      "Training Loss: 0.06227978691458702\n",
      "Training Loss: 0.02492411434650421\n",
      "Training Loss: 0.024058468639850616\n",
      "Training Loss: 0.05925404652953148\n",
      "Training Loss: 0.023347720503807068\n",
      "Training Loss: 0.038626182824373245\n",
      "Training Loss: 0.04188639670610428\n",
      "Training Loss: 0.02725909650325775\n",
      "Training Loss: 0.009897488169372082\n",
      "Training Loss: 0.03942767158150673\n",
      "Training Loss: 0.05661996826529503\n",
      "Training Loss: 0.11085013300180435\n",
      "Training Loss: 0.04889928176999092\n",
      "Training Loss: 0.050082866102457047\n",
      "Training Loss: 0.07357578724622726\n",
      "Training Loss: 0.1902886927127838\n",
      "Training Loss: 0.10406384617090225\n",
      "Training Loss: 0.16739703714847565\n",
      "Test Accuracy: 0.9805825352668762 ---- Test Loss: 0.0747154951095581\n",
      "Epoch time: 5.853879690170288 seconds\n",
      "###### Epoch 22 ######\n",
      "Training Loss: 0.0782790556550026\n",
      "Training Loss: 0.12622329592704773\n",
      "Training Loss: 0.049699049443006516\n",
      "Training Loss: 0.029709873721003532\n",
      "Training Loss: 0.04966800659894943\n",
      "Training Loss: 0.052450861781835556\n",
      "Training Loss: 0.06773888319730759\n",
      "Training Loss: 0.11394695192575455\n",
      "Training Loss: 0.06709137558937073\n",
      "Training Loss: 0.05712224170565605\n",
      "Training Loss: 0.11502201110124588\n",
      "Training Loss: 0.08156507462263107\n",
      "Training Loss: 0.04478317126631737\n",
      "Training Loss: 0.030099833384156227\n",
      "Training Loss: 0.035167451947927475\n",
      "Training Loss: 0.13170239329338074\n",
      "Training Loss: 0.13761316239833832\n",
      "Training Loss: 0.056585486978292465\n",
      "Test Accuracy: 0.9902912378311157 ---- Test Loss: 0.06413474678993225\n",
      "Epoch time: 4.9120073318481445 seconds\n",
      "###### Epoch 23 ######\n",
      "Training Loss: 0.03813140094280243\n",
      "Training Loss: 0.04361468181014061\n",
      "Training Loss: 0.034135814756155014\n",
      "Training Loss: 0.03486538305878639\n",
      "Training Loss: 0.06566891074180603\n",
      "Training Loss: 0.024134229868650436\n",
      "Training Loss: 0.021772928535938263\n",
      "Training Loss: 0.021508483216166496\n",
      "Training Loss: 0.07057957351207733\n",
      "Training Loss: 0.07256009429693222\n",
      "Training Loss: 0.02147550880908966\n",
      "Training Loss: 0.02626165561378002\n",
      "Training Loss: 0.035197287797927856\n",
      "Training Loss: 0.037455931305885315\n",
      "Training Loss: 0.11659134179353714\n",
      "Training Loss: 0.021331148222088814\n",
      "Training Loss: 0.005553886294364929\n",
      "Training Loss: 0.005094044841825962\n",
      "Test Accuracy: 0.9805825352668762 ---- Test Loss: 0.08503513783216476\n",
      "Epoch time: 5.819984197616577 seconds\n",
      "###### Epoch 24 ######\n",
      "Training Loss: 0.03196144849061966\n",
      "Training Loss: 0.04645756632089615\n",
      "Training Loss: 0.004161478485912085\n",
      "Training Loss: 0.004912195727229118\n",
      "Training Loss: 0.029305363073945045\n",
      "Training Loss: 0.04942120239138603\n",
      "Training Loss: 0.005466005764901638\n",
      "Training Loss: 0.025635678321123123\n",
      "Training Loss: 0.015586062334477901\n",
      "Training Loss: 0.012104235589504242\n",
      "Training Loss: 0.040725644677877426\n",
      "Training Loss: 0.01894180290400982\n",
      "Training Loss: 0.002836168510839343\n",
      "Training Loss: 0.016358762979507446\n",
      "Training Loss: 0.02448597177863121\n",
      "Training Loss: 0.04880915582180023\n",
      "Training Loss: 0.05160675570368767\n",
      "Training Loss: 0.003062634263187647\n",
      "Test Accuracy: 0.9854369163513184 ---- Test Loss: 0.05180814862251282\n",
      "Epoch time: 5.837255954742432 seconds\n",
      "###### Epoch 25 ######\n",
      "Training Loss: 0.00847063958644867\n",
      "Training Loss: 0.015265465714037418\n",
      "Training Loss: 0.009536298923194408\n",
      "Training Loss: 0.0750652551651001\n",
      "Training Loss: 0.009203063324093819\n",
      "Training Loss: 0.02046438306570053\n",
      "Training Loss: 0.05565350875258446\n",
      "Training Loss: 0.024871207773685455\n",
      "Training Loss: 0.012921086512506008\n",
      "Training Loss: 0.029430916532874107\n",
      "Training Loss: 0.02708287723362446\n",
      "Training Loss: 0.004521209746599197\n",
      "Training Loss: 0.019629577174782753\n",
      "Training Loss: 0.0305583905428648\n",
      "Training Loss: 0.03985365480184555\n",
      "Training Loss: 0.006873444188386202\n",
      "Training Loss: 0.004812020808458328\n",
      "Training Loss: 0.010680746287107468\n",
      "Test Accuracy: 0.9854369163513184 ---- Test Loss: 0.0613592155277729\n",
      "Epoch time: 5.859986066818237 seconds\n",
      "###### Epoch 26 ######\n",
      "Training Loss: 0.011698081158101559\n",
      "Training Loss: 0.02306460216641426\n",
      "Training Loss: 0.030038751661777496\n",
      "Training Loss: 0.002169655403122306\n",
      "Training Loss: 0.005646339152008295\n",
      "Training Loss: 0.011038192547857761\n",
      "Training Loss: 0.006259935908019543\n",
      "Training Loss: 0.022088687866926193\n",
      "Training Loss: 0.010309687815606594\n",
      "Training Loss: 0.008105006068944931\n",
      "Training Loss: 0.02260456047952175\n",
      "Training Loss: 0.0006568708340637386\n",
      "Training Loss: 0.00022393380641005933\n",
      "Training Loss: 0.001203141757287085\n",
      "Training Loss: 0.0983642116189003\n",
      "Training Loss: 0.030663207173347473\n",
      "Training Loss: 0.02729361690580845\n",
      "Training Loss: 0.003752252785488963\n",
      "Test Accuracy: 0.9805825352668762 ---- Test Loss: 0.045821815729141235\n",
      "Epoch time: 5.852216720581055 seconds\n",
      "###### Epoch 27 ######\n",
      "Training Loss: 0.0011627493659034371\n",
      "Training Loss: 0.003497451776638627\n",
      "Training Loss: 0.004257385153323412\n",
      "Training Loss: 0.005448902025818825\n",
      "Training Loss: 0.02616269886493683\n",
      "Training Loss: 0.012051662430167198\n",
      "Training Loss: 0.03731413930654526\n",
      "Training Loss: 0.010017764754593372\n",
      "Training Loss: 0.03223859891295433\n",
      "Training Loss: 0.01665976271033287\n",
      "Training Loss: 0.011180184781551361\n",
      "Training Loss: 0.016549985855817795\n",
      "Training Loss: 0.005541609134525061\n",
      "Training Loss: 0.003348232014104724\n",
      "Training Loss: 0.009868230670690536\n",
      "Training Loss: 0.014637288637459278\n",
      "Training Loss: 0.0044615804217755795\n",
      "Training Loss: 0.03443920612335205\n",
      "Test Accuracy: 0.9854369163513184 ---- Test Loss: 0.08553233742713928\n",
      "Epoch time: 5.830535888671875 seconds\n",
      "###### Epoch 28 ######\n",
      "Training Loss: 0.007950013503432274\n",
      "Training Loss: 0.00895734503865242\n",
      "Training Loss: 0.0037827035412192345\n",
      "Training Loss: 0.023418664932250977\n",
      "Training Loss: 0.0016360320150852203\n",
      "Training Loss: 0.01051155012100935\n",
      "Training Loss: 0.025889785960316658\n",
      "Training Loss: 0.0017054826021194458\n",
      "Training Loss: 0.005406615789979696\n",
      "Training Loss: 0.016016367822885513\n",
      "Training Loss: 0.006173233967274427\n",
      "Training Loss: 0.005205718334764242\n",
      "Training Loss: 0.04415404796600342\n",
      "Training Loss: 0.0008335919119417667\n",
      "Training Loss: 0.0051103923469781876\n",
      "Training Loss: 0.005655227228999138\n",
      "Training Loss: 0.012101978994905949\n",
      "Training Loss: 0.016191301867365837\n",
      "Test Accuracy: 0.9951456189155579 ---- Test Loss: 0.01584005542099476\n",
      "Epoch time: 5.840453386306763 seconds\n",
      "###### Epoch 29 ######\n",
      "Training Loss: 0.014845218509435654\n",
      "Training Loss: 0.0031978345941752195\n",
      "Training Loss: 0.0073760515078902245\n",
      "Training Loss: 0.005208825692534447\n",
      "Training Loss: 0.0022015669383108616\n",
      "Training Loss: 0.0069519998505711555\n",
      "Training Loss: 0.00141238106880337\n",
      "Training Loss: 0.006429203320294619\n",
      "Training Loss: 0.0208431426435709\n",
      "Training Loss: 0.007220296189188957\n",
      "Training Loss: 0.00046544443466700613\n",
      "Training Loss: 0.000397393450839445\n",
      "Training Loss: 0.01876850426197052\n",
      "Training Loss: 0.01228369865566492\n",
      "Training Loss: 0.0015374618815258145\n",
      "Training Loss: 0.004282619804143906\n",
      "Training Loss: 0.0033264406956732273\n",
      "Training Loss: 0.004146583843976259\n",
      "Test Accuracy: 0.9854369163513184 ---- Test Loss: 0.028941456228494644\n",
      "Epoch time: 5.865898847579956 seconds\n",
      "###### Epoch 30 ######\n",
      "Training Loss: 0.00046409241622313857\n",
      "Training Loss: 0.004370946437120438\n",
      "Training Loss: 0.01001733262091875\n",
      "Training Loss: 0.002073273528367281\n",
      "Training Loss: 0.00023054466873873025\n",
      "Training Loss: 0.000419017014792189\n",
      "Training Loss: 0.00148808176163584\n",
      "Training Loss: 0.003307509934529662\n",
      "Training Loss: 0.002007437637075782\n",
      "Training Loss: 0.003276949282735586\n",
      "Training Loss: 0.003930671606212854\n",
      "Training Loss: 0.0005742566427215934\n",
      "Training Loss: 0.002333281794562936\n",
      "Training Loss: 0.003605510341003537\n",
      "Training Loss: 0.006499084178358316\n",
      "Training Loss: 0.0016488400287926197\n",
      "Training Loss: 0.0008907531155273318\n",
      "Training Loss: 0.0012665071990340948\n",
      "Test Accuracy: 0.9951456189155579 ---- Test Loss: 0.005518712569028139\n",
      "Epoch time: 5.85150408744812 seconds\n"
     ]
    }
   ],
   "source": [
    "losses, test_accs, test_losses, epsilons, alphas, epoch_times = train(BATCH_SIZE, EPOCHS, \n",
    "                                                                      remote_model, remote_torch,\n",
    "                                                                      optim, loss_function, \n",
    "                                                                      train_data_ptr, train_labels_ptr, \n",
    "                                                                      test_data_ptr, test_labels_ptr, \n",
    "                                                                      [1, 64, 64], device, privacy_engine_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9521ae8-ba3b-4697-bc11-4247f07d2b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9829931972789115 ---- Validation Loss: 0.044213008135557175\n"
     ]
    }
   ],
   "source": [
    "from tools import datasets\n",
    "# TODO!: Use best model for validation\n",
    "# Sadly very redundant since remotly tracking best model is not easily possible and therefore just last model is used for validation\n",
    "# Evalutating the model locally with the validation data\n",
    "eval_model = remote_model.get(request_block=True, reason=\"Needed for local evaluation!\")\n",
    "eval_model.cuda(torch.device('cuda:0'))\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Higher sample_size here as on DO side will ensure that it is actually data that is not in train or test set!\n",
    "_, _, val_ds = datasets.Loader.load_MedNIST(sample_size=0.1, test_size=0.1, val_size=0.05)\n",
    "val_data, val_labels = val_ds.as_tensor()\n",
    "\n",
    "val_acc, val_loss = test(eval_model, loss_function, torch, val_data, val_labels, torch.device('cuda:0'))\n",
    "\n",
    "print(f'Validation Accuracy: {val_acc} ---- Validation Loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f07ae45-0590-41e0-a203-1e3c1151307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking all interesting variables and results in .csv file\n",
    "if TRACKING:\n",
    "    d = {\n",
    "        'model': MODEL,\n",
    "        'dataset': DATASET,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epochs': EPOCHS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'train_sample_size': SAMPLE_SIZE,\n",
    "        'test_sample_size': len(test_data_ptr),\n",
    "        'val_sample_size': len(val_data),\n",
    "        'delta': DELTA,\n",
    "        'noise_multiplier': NOISE_MULTIPLIER,\n",
    "        'max_grad_norm': MAX_GRAD_NORM,\n",
    "        'dp_used': DP,\n",
    "        'epsilons': epsilons,\n",
    "        'alphas': alphas,\n",
    "        'train_losses': losses,\n",
    "        'test_accs': test_accs,\n",
    "        'test_losses': test_losses,\n",
    "        'val_acc': val_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'epoch_times': epoch_times\n",
    "    }      \n",
    "    df = pd.read_csv('./Results/1DS.csv')\n",
    "    df = df.append(d, ignore_index=True)\n",
    "    df.to_csv('./Results/1DS.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
