{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4a458-592c-4655-aad9-361965349dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import torch\n",
    "from tools import models\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "sy.load('opacus')\n",
    "np.random.seed(42) # The meaning of life!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30127c81-74c2-4c22-aed8-e01b786a64e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "duet = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e651e4cb-754b-4498-92c5-de1b8ec55fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the pionters to the data\n",
    "train_data_ptr = duet.store[0]\n",
    "train_labels_ptr = duet.store[1]\n",
    "\n",
    "test_data_ptr = duet.store[2]\n",
    "test_labels_ptr = duet.store[3]\n",
    "\n",
    "val_data_ptr = duet.store[4]\n",
    "val_labels_ptr = duet.store[5]\n",
    "\n",
    "val_data = val_data_ptr.get(request_block=True, reason=\"Needed for Validation!\")\n",
    "val_labels = val_labels_ptr.get(request_block=True, reason=\"Needed for Validation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315d603d-da4d-4650-a06c-69411dc72724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for training and Differential Privacy\n",
    "length = len(train_data_ptr)\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 20\n",
    "SAMPLE_SIZE = length - length % BATCH_SIZE # NOTE: Current implementation only trains data in multiples of batch size. So BATCH_SIZE % LENGTH amount of data will not be used for training.\n",
    "SAMPLE_RATE = BATCH_SIZE / SAMPLE_SIZE\n",
    "DELTA = 0.001 # Set to be less then the inverse of the training dataset (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "NOISE_MULTIPLIER = 2.0 # The amount of noise sampled and added to the average of the gradients in a batch (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "MAX_GRAD_NORM = 1.2 # The maximum L2 norm of per-sample gradients before they are aggregated by the averaging step (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "\n",
    "DP = True\n",
    "\n",
    "# Getting remote and local instances\n",
    "local_model = models.Deep2DNet(torch)\n",
    "remote_model = local_model.send(duet)\n",
    "remote_torch = duet.torch\n",
    "remote_opacus = duet.opacus\n",
    "\n",
    "# Setting device to train on\n",
    "cuda_available = remote_torch.cuda.is_available().get(request_block=True, reason='Need to check for available GPU!')\n",
    "if cuda_available:\n",
    "    device = remote_torch.device('cuda:0')\n",
    "    remote_model.cuda(device)\n",
    "else:\n",
    "    device = remote_torch.device('cpu')\n",
    "    remote_model.cpu()\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "params = remote_model.parameters()\n",
    "optim = remote_torch.optim.Adam(params=params, lr=0.003) # without DP: 0.0001 // with DP: 0.002-0.003\n",
    "loss_function = remote_torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Setting up Differential Privacy Engine\n",
    "if DP:\n",
    "    privacy_engine_ptr = remote_opacus.privacy_engine.PrivacyEngine(\n",
    "        remote_model.real_module, sample_rate=SAMPLE_RATE,\n",
    "        noise_multiplier=NOISE_MULTIPLIER, max_grad_norm=MAX_GRAD_NORM\n",
    "    )\n",
    "    privacy_engine_ptr.attach(optim)\n",
    "else:\n",
    "    privacy_engine_ptr = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519d7d2-7dbd-4a04-aab8-8ae0a68dfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(batch_size, epochs, model, \n",
    "          torch_ref, optim, loss_function, \n",
    "          train_data, train_labels, test_data, \n",
    "          test_labels, input_shape, device, privacy_engine=None):\n",
    "    \n",
    "    # Variables to track\n",
    "    losses = [] # Average loss per epoch\n",
    "    test_accs = []\n",
    "    test_losses = []\n",
    "    epsilons = [] \n",
    "    alphas = []\n",
    "    epoch_times = [] # Training times for each epoch\n",
    "    \n",
    "    # Divide dataset into batches (sadly remote DataLoaders aren't yet a thing in pysyft)\n",
    "    length = len(train_data)\n",
    "    \n",
    "    if length % batch_size != 0:\n",
    "        cut_data = train_data[:length - length % batch_size]\n",
    "        cut_labels = train_labels[:length - length % batch_size]\n",
    "        \n",
    "    shape = [-1, batch_size]\n",
    "    shape.extend(input_shape)\n",
    "    \n",
    "    batch_data = cut_data.view(shape)\n",
    "    batch_labels = cut_labels.view(-1, batch_size)\n",
    "    \n",
    "    # Prepare indices for randomization of order for each epoch\n",
    "    indices = np.arange(int(length / batch_size))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        epoch_loss = []\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        print(f'###### Epoch {epoch + 1} ######')\n",
    "        for i in indices:\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            output = model(batch_data[int(i)].to(device))\n",
    "            \n",
    "            loss = loss_function(output, batch_labels[int(i)].to(device))\n",
    "            loss_item = loss.item()\n",
    "            \n",
    "            if model.is_local:\n",
    "                loss_value = loss_item\n",
    "            else:\n",
    "                loss_value = loss_item.get(reason=\"To evaluate training progress\", request_block=True, timeout_secs=5)\n",
    "            print(f'Training Loss: {loss_value}')\n",
    "            epoch_loss.append(loss_value)\n",
    "        \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        # Cheking our privacy budget\n",
    "        if privacy_engine is not None:\n",
    "            epsilon_tuple = privacy_engine.get_privacy_spent(DELTA)\n",
    "            epsilon_ptr = epsilon_tuple[0].resolve_pointer_type()\n",
    "            best_alpha_ptr = epsilon_tuple[1].resolve_pointer_type()\n",
    "\n",
    "            epsilon = epsilon_ptr.get(\n",
    "                reason=\"So we dont go over it\",\n",
    "                request_block=True,\n",
    "                timeout_secs=5\n",
    "            )\n",
    "            best_alpha = best_alpha_ptr.get(\n",
    "                reason=\"So we dont go over it\",\n",
    "                request_block=True,\n",
    "                timeout_secs=5\n",
    "            )\n",
    "            if epsilon is None:\n",
    "                epsilon = float(\"-inf\")\n",
    "            if best_alpha is None:\n",
    "                best_alpha = float(\"-inf\")\n",
    "            print(\n",
    "                f\"(ε = {epsilon:.2f}, δ = {DELTA}) for α = {best_alpha}\"\n",
    "            )\n",
    "            epsilons.append(epsilon)\n",
    "            alphas.append(best_alpha)\n",
    "    \n",
    "        test_acc, test_loss = test(model, loss_function, torch_ref, test_data, test_labels, device)\n",
    "        print(f'Test Accuracy: {test_acc} ---- Test Loss: {test_loss}')\n",
    "        \n",
    "        epoch_end = time.time()\n",
    "        print(f\"Epoch time: {int(epoch_end - epoch_start)} seconds\")\n",
    "        \n",
    "        losses.append(epoch_loss / int(length / batch_size))\n",
    "        epoch_times.append(int(epoch_end - epoch_start))\n",
    "        test_accs.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "    return losses, test_accs, test_losses, epsilons, alphas, epoch_times\n",
    "                   \n",
    "            \n",
    "def test(model, loss_function, torch_ref, data, labels, device):\n",
    "    model.eval()\n",
    "    \n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    length = len(data)\n",
    "    \n",
    "    with torch_ref.no_grad():\n",
    "        output = model(data)\n",
    "        test_loss = loss_function(output, labels)\n",
    "        prediction = output.argmax(dim=1)\n",
    "        total = prediction.eq(labels).sum().item()\n",
    "        \n",
    "    acc_ptr = total / length\n",
    "    if model.is_local:\n",
    "        acc = acc_ptr\n",
    "        loss = test_loss\n",
    "    else:\n",
    "        acc = acc_ptr.get(reason=\"To evaluate training progress\", request_block=True, timeout_secs=5)\n",
    "        loss = test_loss.get(reason=\"To evaluate training progress\", request_block=True, timeout_secs=5)\n",
    "\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983458aa-a990-4722-aba4-9ca3fc9abc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, test_accs, test_losses, epsilons, alphas, epoch_times = train(BATCH_SIZE, EPOCHS, \n",
    "                                                                      remote_model, remote_torch,\n",
    "                                                                      optim, loss_function, \n",
    "                                                                      train_data_ptr, train_labels_ptr, \n",
    "                                                                      test_data_ptr, test_labels_ptr, \n",
    "                                                                      [1, 64, 64], device, privacy_engine_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9521ae8-ba3b-4697-bc11-4247f07d2b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalutating the model locally with the validation data\n",
    "eval_model = remote_model.get(request_block=True, reason=\"Needed for local evaluation!\")\n",
    "\n",
    "val_acc, val_loss = test(eval_model, loss_function, torch, val_data, val_labels, torch.device('cuda:0'))\n",
    "\n",
    "print(f'Validation Accuracy: {val_acc} ---- Validation Loss: {val_loss}')\n",
    "\n",
    "# length = len(val_data)\n",
    "\n",
    "# val_data.to(torch.device('cuda:0'))\n",
    "# val_labels.to(torch.device('cuda:0'))\n",
    "\n",
    "# eval_model = remote_model.get(request_block=True, reason=\"Needed for local evaluation!\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     eval_output = eval_model(val_data)\n",
    "#     test_loss = loss_function(eval_output, val_labels)\n",
    "#     prediction = eval_output.argmax(dim=1)\n",
    "#     total = prediction.eq(val_labels).sum().item()\n",
    "\n",
    "# print(f'Validation Accuracy: {total / length}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
