{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b4a458-592c-4655-aad9-361965349dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import torch\n",
    "from tools import models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "sy.load('opacus')\n",
    "np.random.seed(42) # The meaning of life!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30127c81-74c2-4c22-aed8-e01b786a64e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {
      "image/png": {
       "unconfined": true,
       "width": 400
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎤  🎸  ♪♪♪ Joining Duet ♫♫♫  🎻  🎹\n",
      "\n",
      "♫♫♫ >\u001b[93m DISCLAIMER\u001b[0m: \u001b[1mDuet is an experimental feature currently in beta.\n",
      "♫♫♫ > Use at your own risk.\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "    > ❤️ \u001b[91mLove\u001b[0m \u001b[92mDuet\u001b[0m? \u001b[93mPlease\u001b[0m \u001b[94mconsider\u001b[0m \u001b[95msupporting\u001b[0m \u001b[91mour\u001b[0m \u001b[93mcommunity!\u001b[0m\n",
      "    > https://github.com/sponsors/OpenMined\u001b[1m\n",
      "\n",
      "♫♫♫ > Punching through firewall to OpenGrid Network Node at:\n",
      "♫♫♫ > http://ec2-18-218-7-180.us-east-2.compute.amazonaws.com:5000\n",
      "♫♫♫ >\n",
      "♫♫♫ > ...waiting for response from OpenGrid Network... \n",
      "♫♫♫ > \u001b[92mDONE!\u001b[0m\n",
      "\n",
      "♫♫♫ > \u001b[92mCONNECTED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "duet = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e651e4cb-754b-4498-92c5-de1b8ec55fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the pionters to the data\n",
    "time.sleep(31) # Sleep timer so you can just press restart on both notebooks without caring (might need to be adjusted)\n",
    "\n",
    "train_data_ptr = duet.store[0]\n",
    "train_labels_ptr = duet.store[1]\n",
    "\n",
    "test_data_ptr = duet.store[2]\n",
    "test_labels_ptr = duet.store[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "315d603d-da4d-4650-a06c-69411dc72724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for tracking purposes\n",
    "MODEL = 'Deep2DNet'\n",
    "DATASET = 'MedNIST'\n",
    "TRACKING = False # Whether or not this run should be tracked in the results csv file\n",
    "DP = True # Whether or not Differential Privacy should be applied\n",
    "\n",
    "# Parameters for training and Differential Privacy\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.003 if DP else 0.001\n",
    "\n",
    "DELTA = 1e-4 # Set to be less then the inverse of the size of the training dataset (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "NOISE_MULTIPLIER = 2.0 # The amount of noise sampled and added to the average of the gradients in a batch (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "MAX_GRAD_NORM = 1.2 # The maximum L2 norm of per-sample gradients before they are aggregated by the averaging step (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "\n",
    "length = len(train_data_ptr)\n",
    "SAMPLE_SIZE = length - length % BATCH_SIZE # NOTE: Current implementation only trains data in multiples of batch size. So BATCH_SIZE % LENGTH amount of data will not be used for training.\n",
    "SAMPLE_RATE = BATCH_SIZE / SAMPLE_SIZE\n",
    "\n",
    "# Getting remote and local instances\n",
    "local_model = models.Deep2DNet(torch)\n",
    "remote_model = local_model.send(duet)\n",
    "remote_torch = duet.torch\n",
    "remote_opacus = duet.opacus\n",
    "\n",
    "# Setting device to train on\n",
    "cuda_available = remote_torch.cuda.is_available().get(request_block=True, reason='Need to check for available GPU!')\n",
    "if cuda_available:\n",
    "    device = remote_torch.device('cuda:0')\n",
    "    remote_model.cuda(device)\n",
    "else:\n",
    "    device = remote_torch.device('cpu')\n",
    "    remote_model.cpu()\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "params = remote_model.parameters()\n",
    "optim = remote_torch.optim.Adam(params=params, lr=LEARNING_RATE)\n",
    "loss_function = remote_torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Setting up Differential Privacy Engine\n",
    "if DP:\n",
    "    privacy_engine_ptr = remote_opacus.privacy_engine.PrivacyEngine(\n",
    "        remote_model.real_module, sample_rate=SAMPLE_RATE,\n",
    "        noise_multiplier=NOISE_MULTIPLIER, max_grad_norm=MAX_GRAD_NORM\n",
    "    )\n",
    "    privacy_engine_ptr.attach(optim)\n",
    "else:\n",
    "    privacy_engine_ptr = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "983458aa-a990-4722-aba4-9ca3fc9abc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Epoch 1 ######\n",
      "Training Loss: 1.793002963066101\n",
      "Training Loss: 1.7968255281448364\n",
      "Training Loss: 1.7886155843734741\n",
      "Training Loss: 1.7915027141571045\n",
      "Training Loss: 1.7915934324264526\n",
      "Training Loss: 1.7883790731430054\n",
      "Training Loss: 1.7903386354446411\n",
      "Training Loss: 1.789439082145691\n",
      "Training Loss: 1.7919256687164307\n",
      "Training Loss: 1.7907696962356567\n",
      "Training Loss: 1.7920715808868408\n",
      "Training Loss: 1.7924797534942627\n",
      "Training Loss: 1.7925193309783936\n",
      "Training Loss: 1.7885079383850098\n",
      "Training Loss: 1.7905253171920776\n",
      "Training Loss: 1.7879664897918701\n",
      "Training Loss: 1.7890034914016724\n",
      "Training Loss: 1.7898094654083252\n",
      "(ε = 0.73, δ = 0.0001) for α = 20.0\n",
      "Test Accuracy: 0.1893203854560852 ---- Test Loss: 1.7884632349014282\n",
      "Epoch time: 5.582241058349609 seconds\n",
      "###### Epoch 2 ######\n",
      "Training Loss: 1.7864103317260742\n",
      "Training Loss: 1.7864216566085815\n",
      "Training Loss: 1.7863430976867676\n",
      "Training Loss: 1.7824947834014893\n",
      "Training Loss: 1.7818816900253296\n",
      "Training Loss: 1.7864089012145996\n",
      "Training Loss: 1.7823225259780884\n",
      "Training Loss: 1.7835427522659302\n",
      "Training Loss: 1.7797399759292603\n",
      "Training Loss: 1.776990532875061\n",
      "Training Loss: 1.7753703594207764\n",
      "Training Loss: 1.7757349014282227\n",
      "Training Loss: 1.7690614461898804\n",
      "Training Loss: 1.7683809995651245\n",
      "Training Loss: 1.7556651830673218\n",
      "Training Loss: 1.7518059015274048\n",
      "Training Loss: 1.759150743484497\n",
      "Training Loss: 1.760683298110962\n",
      "(ε = 0.95, δ = 0.0001) for α = 17.0\n",
      "Test Accuracy: 0.4805825352668762 ---- Test Loss: 1.7424391508102417\n",
      "Epoch time: 5.240886211395264 seconds\n",
      "###### Epoch 3 ######\n",
      "Training Loss: 1.7546722888946533\n",
      "Training Loss: 1.7350600957870483\n",
      "Training Loss: 1.7301069498062134\n",
      "Training Loss: 1.7283053398132324\n",
      "Training Loss: 1.707747220993042\n",
      "Training Loss: 1.6961910724639893\n",
      "Training Loss: 1.700076699256897\n",
      "Training Loss: 1.6775383949279785\n",
      "Training Loss: 1.666628122329712\n",
      "Training Loss: 1.647284746170044\n",
      "Training Loss: 1.64143705368042\n",
      "Training Loss: 1.6266047954559326\n",
      "Training Loss: 1.6194578409194946\n",
      "Training Loss: 1.602620244026184\n",
      "Training Loss: 1.5562357902526855\n",
      "Training Loss: 1.5301547050476074\n",
      "Training Loss: 1.4891085624694824\n",
      "Training Loss: 1.5015796422958374\n",
      "(ε = 1.12, δ = 0.0001) for α = 15.0\n",
      "Test Accuracy: 0.7038834691047668 ---- Test Loss: 1.4579838514328003\n",
      "Epoch time: 5.204103469848633 seconds\n",
      "###### Epoch 4 ######\n",
      "Training Loss: 1.4259142875671387\n",
      "Training Loss: 1.4127975702285767\n",
      "Training Loss: 1.4097105264663696\n",
      "Training Loss: 1.3506994247436523\n",
      "Training Loss: 1.4043155908584595\n",
      "Training Loss: 1.3265142440795898\n",
      "Training Loss: 1.303647756576538\n",
      "Training Loss: 1.296504020690918\n",
      "Training Loss: 1.2566828727722168\n",
      "Training Loss: 1.2145071029663086\n",
      "Training Loss: 1.1236844062805176\n",
      "Training Loss: 1.1484280824661255\n",
      "Training Loss: 1.0914746522903442\n",
      "Training Loss: 1.0898040533065796\n",
      "Training Loss: 1.0088341236114502\n",
      "Training Loss: 0.9653778672218323\n",
      "Training Loss: 1.0937261581420898\n",
      "Training Loss: 1.031306505203247\n",
      "(ε = 1.27, δ = 0.0001) for α = 14.0\n",
      "Test Accuracy: 0.6990291476249695 ---- Test Loss: 0.9561415910720825\n",
      "Epoch time: 9.027892589569092 seconds\n",
      "###### Epoch 5 ######\n",
      "Training Loss: 1.0095082521438599\n",
      "Training Loss: 0.9028847217559814\n",
      "Training Loss: 0.8674610257148743\n",
      "Training Loss: 0.8661158680915833\n",
      "Training Loss: 0.8403732180595398\n",
      "Training Loss: 0.8745288848876953\n",
      "Training Loss: 0.7759845852851868\n",
      "Training Loss: 0.7804581522941589\n",
      "Training Loss: 0.7424057126045227\n",
      "Training Loss: 0.8961756825447083\n",
      "Training Loss: 0.6779561042785645\n",
      "Training Loss: 0.7625117301940918\n",
      "Training Loss: 0.7930654883384705\n",
      "Training Loss: 0.6999939680099487\n",
      "Training Loss: 0.8076584339141846\n",
      "Training Loss: 0.841930091381073\n",
      "Training Loss: 0.9247713685035706\n",
      "Training Loss: 0.6670987606048584\n",
      "(ε = 1.40, δ = 0.0001) for α = 13.0\n",
      "Test Accuracy: 0.7718446850776672 ---- Test Loss: 0.735115110874176\n",
      "Epoch time: 8.057945966720581 seconds\n",
      "###### Epoch 6 ######\n",
      "Training Loss: 0.6698219776153564\n",
      "Training Loss: 0.695444643497467\n",
      "Training Loss: 0.600250244140625\n",
      "Training Loss: 0.759829580783844\n",
      "Training Loss: 0.8943454623222351\n",
      "Training Loss: 0.9957482218742371\n",
      "Training Loss: 0.7413524389266968\n",
      "Training Loss: 0.6682155728340149\n",
      "Training Loss: 0.8868435621261597\n",
      "Training Loss: 0.4590550363063812\n",
      "Training Loss: 0.6574140787124634\n",
      "Training Loss: 0.9102078080177307\n",
      "Training Loss: 0.44323766231536865\n",
      "Training Loss: 0.58164381980896\n",
      "Training Loss: 0.6234026551246643\n",
      "Training Loss: 0.5334943532943726\n",
      "Training Loss: 0.742506206035614\n",
      "Training Loss: 0.717715322971344\n",
      "(ε = 1.52, δ = 0.0001) for α = 12.0\n",
      "Test Accuracy: 0.7961165308952332 ---- Test Loss: 0.7053170204162598\n",
      "Epoch time: 7.133265972137451 seconds\n",
      "###### Epoch 7 ######\n",
      "Training Loss: 0.7004563808441162\n",
      "Training Loss: 1.0209922790527344\n",
      "Training Loss: 0.8562002778053284\n",
      "Training Loss: 0.7846418619155884\n",
      "Training Loss: 0.9376773238182068\n",
      "Training Loss: 0.6558734774589539\n",
      "Training Loss: 0.5477702021598816\n",
      "Training Loss: 0.7052149176597595\n",
      "Training Loss: 0.9007607102394104\n",
      "Training Loss: 0.8886016011238098\n",
      "Training Loss: 0.6493202447891235\n",
      "Training Loss: 0.7797213792800903\n",
      "Training Loss: 0.7241396903991699\n",
      "Training Loss: 0.5795427560806274\n",
      "Training Loss: 0.7352946400642395\n",
      "Training Loss: 0.4166841506958008\n",
      "Training Loss: 0.6681416034698486\n",
      "Training Loss: 0.8377822041511536\n",
      "(ε = 1.64, δ = 0.0001) for α = 12.0\n",
      "Test Accuracy: 0.8058252334594727 ---- Test Loss: 0.7153356075286865\n",
      "Epoch time: 5.201860189437866 seconds\n",
      "###### Epoch 8 ######\n",
      "Training Loss: 0.8075218200683594\n",
      "Training Loss: 0.9319468140602112\n",
      "Training Loss: 1.487917423248291\n",
      "Training Loss: 1.2737219333648682\n",
      "Training Loss: 1.1865566968917847\n",
      "Training Loss: 0.8331363201141357\n",
      "Training Loss: 0.5355933904647827\n",
      "Training Loss: 0.5654815435409546\n",
      "Training Loss: 0.7965337634086609\n",
      "Training Loss: 0.5199155211448669\n",
      "Training Loss: 0.7585544586181641\n",
      "Training Loss: 0.724163293838501\n",
      "Training Loss: 0.6576526761054993\n",
      "Training Loss: 0.8251892328262329\n",
      "Training Loss: 1.4965401887893677\n",
      "Training Loss: 2.1824724674224854\n",
      "Training Loss: 1.3807145357131958\n",
      "Training Loss: 0.9253631830215454\n",
      "(ε = 1.74, δ = 0.0001) for α = 10.899999618530273\n",
      "Test Accuracy: 0.6796116232872009 ---- Test Loss: 1.27013099193573\n",
      "Epoch time: 5.233702182769775 seconds\n",
      "###### Epoch 9 ######\n",
      "Training Loss: 1.1966185569763184\n",
      "Training Loss: 1.205357551574707\n",
      "Training Loss: 1.3483726978302002\n",
      "Training Loss: 0.904461681842804\n",
      "Training Loss: 0.5996463894844055\n",
      "Training Loss: 0.47246289253234863\n",
      "Training Loss: 1.1120387315750122\n",
      "Training Loss: 1.3436018228530884\n",
      "Training Loss: 0.8940077424049377\n",
      "Training Loss: 1.4909814596176147\n",
      "Training Loss: 1.119511604309082\n",
      "Training Loss: 0.7270475029945374\n",
      "Training Loss: 0.8166776895523071\n",
      "Training Loss: 0.6564115881919861\n",
      "Training Loss: 0.8545935750007629\n",
      "Training Loss: 0.7666601538658142\n",
      "Training Loss: 0.36608627438545227\n",
      "Training Loss: 0.5494975447654724\n",
      "(ε = 1.84, δ = 0.0001) for α = 10.5\n",
      "Test Accuracy: 0.7572815418243408 ---- Test Loss: 0.9596445560455322\n",
      "Epoch time: 6.143993139266968 seconds\n",
      "###### Epoch 10 ######\n",
      "Training Loss: 0.6566142439842224\n",
      "Training Loss: 0.8987849950790405\n",
      "Training Loss: 1.0712127685546875\n",
      "Training Loss: 0.3283829391002655\n",
      "Training Loss: 0.6796351671218872\n",
      "Training Loss: 1.2671715021133423\n",
      "Training Loss: 0.4608801305294037\n",
      "Training Loss: 0.6756823658943176\n",
      "Training Loss: 0.6297120451927185\n",
      "Training Loss: 0.6168969869613647\n",
      "Training Loss: 0.7506430149078369\n",
      "Training Loss: 0.44176819920539856\n",
      "Training Loss: 0.7697854042053223\n",
      "Training Loss: 0.7973676919937134\n",
      "Training Loss: 0.962923526763916\n",
      "Training Loss: 0.7626916766166687\n",
      "Training Loss: 1.3275611400604248\n",
      "Training Loss: 0.7366035580635071\n",
      "(ε = 1.94, δ = 0.0001) for α = 10.100000381469727\n",
      "Test Accuracy: 0.8786407709121704 ---- Test Loss: 0.7286694645881653\n",
      "Epoch time: 5.203991413116455 seconds\n",
      "###### Epoch 11 ######\n",
      "Training Loss: 0.8467807173728943\n",
      "Training Loss: 0.7597509026527405\n",
      "Training Loss: 1.0220844745635986\n",
      "Training Loss: 1.0076937675476074\n",
      "Training Loss: 0.6628980040550232\n",
      "Training Loss: 1.203606128692627\n",
      "Training Loss: 0.6430966258049011\n",
      "Training Loss: 1.2338647842407227\n",
      "Training Loss: 0.583809494972229\n",
      "Training Loss: 0.5793951153755188\n",
      "Training Loss: 0.5069168210029602\n",
      "Training Loss: 0.6847283840179443\n",
      "Training Loss: 0.789582371711731\n",
      "Training Loss: 1.187366247177124\n",
      "Training Loss: 0.9550478458404541\n",
      "Training Loss: 0.666515588760376\n",
      "Training Loss: 0.771773099899292\n",
      "Training Loss: 0.6069201827049255\n",
      "(ε = 2.03, δ = 0.0001) for α = 9.800000190734863\n",
      "Test Accuracy: 0.7815533876419067 ---- Test Loss: 1.0069907903671265\n",
      "Epoch time: 5.195919513702393 seconds\n",
      "###### Epoch 12 ######\n",
      "Training Loss: 0.6818233728408813\n",
      "Training Loss: 1.0075243711471558\n",
      "Training Loss: 0.8360698819160461\n",
      "Training Loss: 0.82367342710495\n",
      "Training Loss: 0.6511805057525635\n",
      "Training Loss: 1.288262128829956\n",
      "Training Loss: 1.4259272813796997\n",
      "Training Loss: 0.9028101563453674\n",
      "Training Loss: 1.1337957382202148\n",
      "Training Loss: 0.7061921954154968\n",
      "Training Loss: 0.794106125831604\n",
      "Training Loss: 0.7984079122543335\n",
      "Training Loss: 2.001276969909668\n",
      "Training Loss: 1.412516474723816\n",
      "Training Loss: 0.7765036821365356\n",
      "Training Loss: 1.3550969362258911\n",
      "Training Loss: 0.7895230054855347\n",
      "Training Loss: 0.9663974642753601\n",
      "(ε = 2.11, δ = 0.0001) for α = 9.399999618530273\n",
      "Test Accuracy: 0.8592233061790466 ---- Test Loss: 0.9898819923400879\n",
      "Epoch time: 9.041377305984497 seconds\n",
      "###### Epoch 13 ######\n",
      "Training Loss: 1.1493430137634277\n",
      "Training Loss: 1.0375713109970093\n",
      "Training Loss: 0.7368115782737732\n",
      "Training Loss: 1.3138606548309326\n",
      "Training Loss: 1.7637370824813843\n",
      "Training Loss: 1.0070407390594482\n",
      "Training Loss: 0.61126708984375\n",
      "Training Loss: 1.3724288940429688\n",
      "Training Loss: 0.6048886775970459\n",
      "Training Loss: 0.7608015537261963\n",
      "Training Loss: 0.8331339359283447\n",
      "Training Loss: 1.1281477212905884\n",
      "Training Loss: 1.6626214981079102\n",
      "Training Loss: 0.9933558702468872\n",
      "Training Loss: 1.6840213537216187\n",
      "Training Loss: 1.39056396484375\n",
      "Training Loss: 1.0556683540344238\n",
      "Training Loss: 0.6259161233901978\n",
      "(ε = 2.20, δ = 0.0001) for α = 9.199999809265137\n",
      "Test Accuracy: 0.8543689250946045 ---- Test Loss: 1.1171263456344604\n",
      "Epoch time: 5.2246856689453125 seconds\n",
      "###### Epoch 14 ######\n",
      "Training Loss: 1.271294116973877\n",
      "Training Loss: 1.0237501859664917\n",
      "Training Loss: 1.1306370496749878\n",
      "Training Loss: 1.0158120393753052\n",
      "Training Loss: 0.7461354732513428\n",
      "Training Loss: 1.309279441833496\n",
      "Training Loss: 0.7104697227478027\n",
      "Training Loss: 1.9388775825500488\n",
      "Training Loss: 0.8135522603988647\n",
      "Training Loss: 0.7574947476387024\n",
      "Training Loss: 1.0898514986038208\n",
      "Training Loss: 0.49740543961524963\n",
      "Training Loss: 1.5746818780899048\n",
      "Training Loss: 0.7152672410011292\n",
      "Training Loss: 0.6165907979011536\n",
      "Training Loss: 0.8067678213119507\n",
      "Training Loss: 1.387131929397583\n",
      "Training Loss: 1.852689504623413\n",
      "(ε = 2.28, δ = 0.0001) for α = 8.899999618530273\n",
      "Test Accuracy: 0.8203883767127991 ---- Test Loss: 1.661325454711914\n",
      "Epoch time: 6.162964105606079 seconds\n",
      "###### Epoch 15 ######\n",
      "Training Loss: 1.8678102493286133\n",
      "Training Loss: 1.8905624151229858\n",
      "Training Loss: 1.3129311800003052\n",
      "Training Loss: 1.0093499422073364\n",
      "Training Loss: 0.7516212463378906\n",
      "Training Loss: 1.0391530990600586\n",
      "Training Loss: 1.8531533479690552\n",
      "Training Loss: 1.7905877828598022\n",
      "Training Loss: 1.597435712814331\n",
      "Training Loss: 0.49700304865837097\n",
      "Training Loss: 0.8584542870521545\n",
      "Training Loss: 1.1516016721725464\n",
      "Training Loss: 1.7482914924621582\n",
      "Training Loss: 0.9688770771026611\n",
      "Training Loss: 1.1046830415725708\n",
      "Training Loss: 1.4842654466629028\n",
      "Training Loss: 1.254199504852295\n",
      "Training Loss: 1.109580159187317\n",
      "(ε = 2.36, δ = 0.0001) for α = 8.699999809265137\n",
      "Test Accuracy: 0.7961165308952332 ---- Test Loss: 1.9877148866653442\n",
      "Epoch time: 6.185263633728027 seconds\n",
      "###### Epoch 16 ######\n",
      "Training Loss: 1.014252781867981\n",
      "Training Loss: 0.6952057480812073\n",
      "Training Loss: 1.4600979089736938\n",
      "Training Loss: 1.6175248622894287\n",
      "Training Loss: 1.3444448709487915\n",
      "Training Loss: 1.2671149969100952\n",
      "Training Loss: 1.2433053255081177\n",
      "Training Loss: 2.0382044315338135\n",
      "Training Loss: 1.103650450706482\n",
      "Training Loss: 1.4764831066131592\n",
      "Training Loss: 2.2867865562438965\n",
      "Training Loss: 1.3955835103988647\n",
      "Training Loss: 1.7466005086898804\n",
      "Training Loss: 1.1910147666931152\n",
      "Training Loss: 1.372949242591858\n",
      "Training Loss: 2.131455421447754\n",
      "Training Loss: 1.501867651939392\n",
      "Training Loss: 1.9478853940963745\n",
      "(ε = 2.43, δ = 0.0001) for α = 8.5\n",
      "Test Accuracy: 0.8106796145439148 ---- Test Loss: 1.8277746438980103\n",
      "Epoch time: 7.142928123474121 seconds\n",
      "###### Epoch 17 ######\n",
      "Training Loss: 1.5124396085739136\n",
      "Training Loss: 1.1050703525543213\n",
      "Training Loss: 1.4183710813522339\n",
      "Training Loss: 1.2053477764129639\n",
      "Training Loss: 1.322505235671997\n",
      "Training Loss: 1.5246996879577637\n",
      "Training Loss: 1.7384964227676392\n",
      "Training Loss: 2.049380302429199\n",
      "Training Loss: 1.9253144264221191\n",
      "Training Loss: 1.5491538047790527\n",
      "Training Loss: 2.055717706680298\n",
      "Training Loss: 1.1431093215942383\n",
      "Training Loss: 2.137935161590576\n",
      "Training Loss: 1.3249636888504028\n",
      "Training Loss: 2.327629327774048\n",
      "Training Loss: 1.390312910079956\n",
      "Training Loss: 1.9955530166625977\n",
      "Training Loss: 2.290890693664551\n",
      "(ε = 2.51, δ = 0.0001) for α = 8.300000190734863\n",
      "Test Accuracy: 0.7718446850776672 ---- Test Loss: 2.0148091316223145\n",
      "Epoch time: 5.273190498352051 seconds\n",
      "###### Epoch 18 ######\n",
      "Training Loss: 1.8988674879074097\n",
      "Training Loss: 1.333044409751892\n",
      "Training Loss: 1.3676958084106445\n",
      "Training Loss: 1.4556686878204346\n",
      "Training Loss: 1.682985544204712\n",
      "Training Loss: 1.3775315284729004\n",
      "Training Loss: 1.2731049060821533\n",
      "Training Loss: 2.2600514888763428\n",
      "Training Loss: 0.7124968767166138\n",
      "Training Loss: 2.387194871902466\n",
      "Training Loss: 1.5122548341751099\n",
      "Training Loss: 1.5788325071334839\n",
      "Training Loss: 2.300095319747925\n",
      "Training Loss: 1.4581162929534912\n",
      "Training Loss: 2.1220223903656006\n",
      "Training Loss: 0.9939879775047302\n",
      "Training Loss: 1.518527626991272\n",
      "Training Loss: 1.0884649753570557\n",
      "(ε = 2.58, δ = 0.0001) for α = 8.100000381469727\n",
      "Test Accuracy: 0.844660222530365 ---- Test Loss: 1.9852237701416016\n",
      "Epoch time: 5.242777347564697 seconds\n",
      "###### Epoch 19 ######\n",
      "Training Loss: 1.9312268495559692\n",
      "Training Loss: 1.938594937324524\n",
      "Training Loss: 2.2539260387420654\n",
      "Training Loss: 1.3210335969924927\n",
      "Training Loss: 1.4542655944824219\n",
      "Training Loss: 1.5240668058395386\n",
      "Training Loss: 1.9640579223632812\n",
      "Training Loss: 2.790018081665039\n",
      "Training Loss: 1.3182569742202759\n",
      "Training Loss: 1.4809449911117554\n",
      "Training Loss: 1.8805943727493286\n",
      "Training Loss: 0.7942647337913513\n",
      "Training Loss: 1.3585789203643799\n",
      "Training Loss: 1.8839654922485352\n",
      "Training Loss: 1.1572850942611694\n",
      "Training Loss: 1.8980122804641724\n",
      "Training Loss: 1.8683379888534546\n",
      "Training Loss: 2.211150646209717\n",
      "(ε = 2.65, δ = 0.0001) for α = 7.900000095367432\n",
      "Test Accuracy: 0.7864077687263489 ---- Test Loss: 2.5410542488098145\n",
      "Epoch time: 8.030783891677856 seconds\n",
      "###### Epoch 20 ######\n",
      "Training Loss: 1.2190344333648682\n",
      "Training Loss: 1.842525601387024\n",
      "Training Loss: 1.7859991788864136\n",
      "Training Loss: 2.9507009983062744\n",
      "Training Loss: 2.4320855140686035\n",
      "Training Loss: 1.2179948091506958\n",
      "Training Loss: 1.7324882745742798\n",
      "Training Loss: 1.1764332056045532\n",
      "Training Loss: 1.2432758808135986\n",
      "Training Loss: 1.0057220458984375\n",
      "Training Loss: 1.967933177947998\n",
      "Training Loss: 1.691768765449524\n",
      "Training Loss: 1.6466102600097656\n",
      "Training Loss: 1.4938304424285889\n",
      "Training Loss: 1.2664849758148193\n",
      "Training Loss: 2.643057346343994\n",
      "Training Loss: 1.8742669820785522\n",
      "Training Loss: 2.273855209350586\n",
      "(ε = 2.72, δ = 0.0001) for α = 7.800000190734863\n",
      "Test Accuracy: 0.7524271607398987 ---- Test Loss: 2.7612180709838867\n",
      "Epoch time: 7.099977970123291 seconds\n",
      "###### Epoch 21 ######\n",
      "Training Loss: 1.0457978248596191\n",
      "Training Loss: 2.2384064197540283\n",
      "Training Loss: 1.5125030279159546\n",
      "Training Loss: 2.6060972213745117\n",
      "Training Loss: 1.8905616998672485\n",
      "Training Loss: 3.1345150470733643\n",
      "Training Loss: 1.5761680603027344\n",
      "Training Loss: 1.6842560768127441\n",
      "Training Loss: 2.4497711658477783\n",
      "Training Loss: 3.1938889026641846\n",
      "Training Loss: 2.8553197383880615\n",
      "Training Loss: 4.319389820098877\n",
      "Training Loss: 2.8761768341064453\n",
      "Training Loss: 2.9561917781829834\n",
      "Training Loss: 2.4235007762908936\n",
      "Training Loss: 1.9794092178344727\n",
      "Training Loss: 1.3841981887817383\n",
      "Training Loss: 1.4736279249191284\n",
      "(ε = 2.78, δ = 0.0001) for α = 7.599999904632568\n",
      "Test Accuracy: 0.7135922312736511 ---- Test Loss: 2.9006714820861816\n",
      "Epoch time: 5.222963571548462 seconds\n",
      "###### Epoch 22 ######\n",
      "Training Loss: 2.4616756439208984\n",
      "Training Loss: 2.4029860496520996\n",
      "Training Loss: 2.4966914653778076\n",
      "Training Loss: 2.6749236583709717\n",
      "Training Loss: 2.4366791248321533\n",
      "Training Loss: 2.529738187789917\n",
      "Training Loss: 2.3088889122009277\n",
      "Training Loss: 3.269582748413086\n",
      "Training Loss: 1.1942766904830933\n",
      "Training Loss: 0.8236053586006165\n",
      "Training Loss: 3.548633337020874\n",
      "Training Loss: 1.74202299118042\n",
      "Training Loss: 2.2685720920562744\n",
      "Training Loss: 2.376492977142334\n",
      "Training Loss: 3.887702703475952\n",
      "Training Loss: 4.205615520477295\n",
      "Training Loss: 2.50209379196167\n",
      "Training Loss: 3.2154242992401123\n",
      "(ε = 2.85, δ = 0.0001) for α = 7.5\n",
      "Test Accuracy: 0.7184466123580933 ---- Test Loss: 3.393956422805786\n",
      "Epoch time: 6.156324148178101 seconds\n",
      "###### Epoch 23 ######\n",
      "Training Loss: 1.8715910911560059\n",
      "Training Loss: 2.8628957271575928\n",
      "Training Loss: 2.8088858127593994\n",
      "Training Loss: 2.339637041091919\n",
      "Training Loss: 4.567502975463867\n",
      "Training Loss: 2.9440670013427734\n",
      "Training Loss: 2.599606990814209\n",
      "Training Loss: 3.3412139415740967\n",
      "Training Loss: 4.019343852996826\n",
      "Training Loss: 2.7353508472442627\n",
      "Training Loss: 2.693349838256836\n",
      "Training Loss: 4.6757941246032715\n",
      "Training Loss: 2.6331210136413574\n",
      "Training Loss: 3.072894334793091\n",
      "Training Loss: 2.88724946975708\n",
      "Training Loss: 5.327264785766602\n",
      "Training Loss: 2.3461828231811523\n",
      "Training Loss: 2.5539984703063965\n",
      "(ε = 2.91, δ = 0.0001) for α = 7.400000095367432\n",
      "Test Accuracy: 0.7572815418243408 ---- Test Loss: 3.3170876502990723\n",
      "Epoch time: 7.143711805343628 seconds\n",
      "###### Epoch 24 ######\n",
      "Training Loss: 3.0161614418029785\n",
      "Training Loss: 2.0263655185699463\n",
      "Training Loss: 4.059109687805176\n",
      "Training Loss: 2.953669548034668\n",
      "Training Loss: 2.1300294399261475\n",
      "Training Loss: 5.797494888305664\n",
      "Training Loss: 0.9437863826751709\n",
      "Training Loss: 2.5974395275115967\n",
      "Training Loss: 3.130242347717285\n",
      "Training Loss: 2.7895848751068115\n",
      "Training Loss: 2.3687453269958496\n",
      "Training Loss: 1.996344804763794\n",
      "Training Loss: 3.558382272720337\n",
      "Training Loss: 2.502028703689575\n",
      "Training Loss: 2.4461874961853027\n",
      "Training Loss: 4.311134338378906\n",
      "Training Loss: 2.7412328720092773\n",
      "Training Loss: 3.304780960083008\n",
      "(ε = 2.98, δ = 0.0001) for α = 7.300000190734863\n",
      "Test Accuracy: 0.7961165308952332 ---- Test Loss: 3.6690242290496826\n",
      "Epoch time: 7.219587564468384 seconds\n",
      "###### Epoch 25 ######\n",
      "Training Loss: 3.1512670516967773\n",
      "Training Loss: 3.2861762046813965\n",
      "Training Loss: 2.433871269226074\n",
      "Training Loss: 2.5418272018432617\n",
      "Training Loss: 3.197812557220459\n",
      "Training Loss: 4.005514621734619\n",
      "Training Loss: 2.7802186012268066\n",
      "Training Loss: 3.647871732711792\n",
      "Training Loss: 4.436578273773193\n",
      "Training Loss: 4.087863445281982\n",
      "Training Loss: 1.4755717515945435\n",
      "Training Loss: 3.106816053390503\n",
      "Training Loss: 2.654846429824829\n",
      "Training Loss: 4.066801071166992\n",
      "Training Loss: 2.2218360900878906\n",
      "Training Loss: 4.512940883636475\n",
      "Training Loss: 4.122104644775391\n",
      "Training Loss: 4.042639255523682\n",
      "(ε = 3.04, δ = 0.0001) for α = 7.099999904632568\n",
      "Test Accuracy: 0.7864077687263489 ---- Test Loss: 4.174895286560059\n",
      "Epoch time: 6.130530834197998 seconds\n",
      "###### Epoch 26 ######\n",
      "Training Loss: 4.181081295013428\n",
      "Training Loss: 3.1487317085266113\n",
      "Training Loss: 3.939274549484253\n",
      "Training Loss: 4.473336696624756\n",
      "Training Loss: 4.273959159851074\n",
      "Training Loss: 3.544463872909546\n",
      "Training Loss: 2.6321945190429688\n",
      "Training Loss: 5.118735313415527\n",
      "Training Loss: 3.8243298530578613\n",
      "Training Loss: 3.3826467990875244\n",
      "Training Loss: 2.558030605316162\n",
      "Training Loss: 4.371540069580078\n",
      "Training Loss: 4.560154438018799\n",
      "Training Loss: 4.743182182312012\n",
      "Training Loss: 3.0189874172210693\n",
      "Training Loss: 4.3006510734558105\n",
      "Training Loss: 6.151432991027832\n",
      "Training Loss: 6.197409152984619\n",
      "(ε = 3.10, δ = 0.0001) for α = 7.0\n",
      "Test Accuracy: 0.7233009934425354 ---- Test Loss: 5.541979789733887\n",
      "Epoch time: 6.341810703277588 seconds\n",
      "###### Epoch 27 ######\n",
      "Training Loss: 5.98983907699585\n",
      "Training Loss: 2.7807106971740723\n",
      "Training Loss: 5.984210968017578\n",
      "Training Loss: 3.9006991386413574\n",
      "Training Loss: 4.850618839263916\n",
      "Training Loss: 3.145359992980957\n",
      "Training Loss: 3.407888889312744\n",
      "Training Loss: 4.237541675567627\n",
      "Training Loss: 4.295079231262207\n",
      "Training Loss: 2.2046282291412354\n",
      "Training Loss: 2.28564453125\n",
      "Training Loss: 3.135765314102173\n",
      "Training Loss: 4.88923978805542\n",
      "Training Loss: 3.7738709449768066\n",
      "Training Loss: 3.6590311527252197\n",
      "Training Loss: 5.883244514465332\n",
      "Training Loss: 2.2400577068328857\n",
      "Training Loss: 5.777409076690674\n",
      "(ε = 3.16, δ = 0.0001) for α = 6.900000095367432\n",
      "Test Accuracy: 0.7281553149223328 ---- Test Loss: 4.4274749755859375\n",
      "Epoch time: 5.175990104675293 seconds\n",
      "###### Epoch 28 ######\n",
      "Training Loss: 5.9079790115356445\n",
      "Training Loss: 4.999854564666748\n",
      "Training Loss: 4.60294246673584\n",
      "Training Loss: 2.5308008193969727\n",
      "Training Loss: 2.444699287414551\n",
      "Training Loss: 4.223818778991699\n",
      "Training Loss: 3.6466481685638428\n",
      "Training Loss: 5.453561782836914\n",
      "Training Loss: 2.6693389415740967\n",
      "Training Loss: 5.245291233062744\n",
      "Training Loss: 7.402630805969238\n",
      "Training Loss: 6.098268985748291\n",
      "Training Loss: 2.992654323577881\n",
      "Training Loss: 3.4201653003692627\n",
      "Training Loss: 5.07247257232666\n",
      "Training Loss: 5.092909336090088\n",
      "Training Loss: 5.097901821136475\n",
      "Training Loss: 3.578497886657715\n",
      "(ε = 3.22, δ = 0.0001) for α = 6.800000190734863\n",
      "Test Accuracy: 0.7184466123580933 ---- Test Loss: 4.687500476837158\n",
      "Epoch time: 6.213181018829346 seconds\n",
      "###### Epoch 29 ######\n",
      "Training Loss: 5.937976837158203\n",
      "Training Loss: 5.104303359985352\n",
      "Training Loss: 5.748840808868408\n",
      "Training Loss: 6.449214458465576\n",
      "Training Loss: 5.176687240600586\n",
      "Training Loss: 5.901917934417725\n",
      "Training Loss: 4.015892028808594\n",
      "Training Loss: 3.6057257652282715\n",
      "Training Loss: 8.762375831604004\n",
      "Training Loss: 3.1757562160491943\n",
      "Training Loss: 5.401602745056152\n",
      "Training Loss: 7.2489471435546875\n",
      "Training Loss: 5.102769374847412\n",
      "Training Loss: 8.901004791259766\n",
      "Training Loss: 5.543422698974609\n",
      "Training Loss: 6.138946056365967\n",
      "Training Loss: 6.669283866882324\n",
      "Training Loss: 6.324763774871826\n",
      "(ε = 3.28, δ = 0.0001) for α = 6.699999809265137\n",
      "Test Accuracy: 0.8106796145439148 ---- Test Loss: 3.3864259719848633\n",
      "Epoch time: 5.206007242202759 seconds\n",
      "###### Epoch 30 ######\n",
      "Training Loss: 4.625522613525391\n",
      "Training Loss: 2.769447088241577\n",
      "Training Loss: 7.586951732635498\n",
      "Training Loss: 4.167057037353516\n",
      "Training Loss: 6.345559597015381\n",
      "Training Loss: 8.713552474975586\n",
      "Training Loss: 5.753122329711914\n",
      "Training Loss: 4.799757957458496\n",
      "Training Loss: 4.322973728179932\n",
      "Training Loss: 4.766705513000488\n",
      "Training Loss: 5.149272441864014\n",
      "Training Loss: 5.524320602416992\n",
      "Training Loss: 3.9825940132141113\n",
      "Training Loss: 6.157656669616699\n",
      "Training Loss: 6.252377986907959\n",
      "Training Loss: 5.809449672698975\n",
      "Training Loss: 6.358333110809326\n",
      "Training Loss: 3.88765025138855\n",
      "(ε = 3.33, δ = 0.0001) for α = 6.699999809265137\n",
      "Test Accuracy: 0.708737850189209 ---- Test Loss: 7.655807971954346\n",
      "Epoch time: 6.157841682434082 seconds\n"
     ]
    }
   ],
   "source": [
    "from tools.utils import train, test\n",
    "\n",
    "losses, test_accs, test_losses, epsilons, alphas, epoch_times = train(BATCH_SIZE, EPOCHS, DELTA, \n",
    "                                                                      remote_model, remote_torch,\n",
    "                                                                      optim, loss_function, \n",
    "                                                                      train_data_ptr, train_labels_ptr, \n",
    "                                                                      test_data_ptr, test_labels_ptr, \n",
    "                                                                      [1, 64, 64], device, privacy_engine_ptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9521ae8-ba3b-4697-bc11-4247f07d2b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6292517006802721 ---- Validation Loss: 9.86394214630127\n"
     ]
    }
   ],
   "source": [
    "from tools import datasets\n",
    "# TODO!: Use best model for validation\n",
    "# Sadly very redundant since remotly tracking best model is not easily possible and therefore just last model is used for validation\n",
    "# Evalutating the model locally with the validation data\n",
    "eval_model = remote_model.get(request_block=True, reason=\"Needed for local evaluation!\")\n",
    "eval_model.cuda(torch.device('cuda:0'))\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Higher sample_size here as on DO side will ensure that it is actually data that is not in train or test set!\n",
    "_, _, val_ds = datasets.Loader.load_MedNIST(sample_size=0.1, test_size=0.1, val_size=0.05)\n",
    "val_data, val_labels = val_ds.as_tensor()\n",
    "\n",
    "val_acc, val_loss = test(eval_model, loss_function, torch, val_data, val_labels, torch.device('cuda:0'))\n",
    "\n",
    "print(f'Validation Accuracy: {val_acc} ---- Validation Loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f07ae45-0590-41e0-a203-1e3c1151307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking all interesting variables and results in .csv file\n",
    "if TRACKING:\n",
    "    d = {\n",
    "        'model': MODEL,\n",
    "        'dataset': DATASET,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epochs': EPOCHS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'train_sample_size': SAMPLE_SIZE,\n",
    "        'test_sample_size': len(test_data_ptr),\n",
    "        'val_sample_size': len(val_data),\n",
    "        'delta': DELTA,\n",
    "        'noise_multiplier': NOISE_MULTIPLIER,\n",
    "        'max_grad_norm': MAX_GRAD_NORM,\n",
    "        'dp_used': DP,\n",
    "        'epsilons': epsilons,\n",
    "        'alphas': alphas,\n",
    "        'train_losses': losses,\n",
    "        'test_accs': test_accs,\n",
    "        'test_losses': test_losses,\n",
    "        'val_acc': val_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'epoch_times': epoch_times\n",
    "    }      \n",
    "    df = pd.read_csv('./Results/1DO-1DS.csv')\n",
    "    df = df.append(d, ignore_index=True)\n",
    "    df.to_csv('./Results/1DO-1DS.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
