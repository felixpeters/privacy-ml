{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5be19f8-647f-4a09-a6d2-d4aaf14f8eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import torch\n",
    "from tools import models, datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import opacus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84708bc9-2182-4493-a122-f44ffb56d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds, val_ds = datasets.Loader.load_MedNIST(sample_size=0.02, test_size=0.1, val_size=0.1)\n",
    "train_data, train_labels = train_ds.as_tensor()\n",
    "test_data, test_labels = test_ds.as_tensor()\n",
    "val_data, val_labels = val_ds.as_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1378933c-c903-4df4-a809-5d460951968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for tracking purposes\n",
    "MODEL = 'Deep2DNet'\n",
    "DATASET = 'MedNIST'\n",
    "TRACKING = True # Whether or not this run should be tracked in the results.csv\n",
    "DP = True # Whether or not Differential Privacy should be applied\n",
    "\n",
    "# Parameters for training and Differential Privacy\n",
    "length = len(train_data)\n",
    "SAMPLE_SIZE = length - length % BATCH_SIZE # NOTE: Current implementation only trains data in multiples of batch size. So BATCH_SIZE % LENGTH amount of data will not be used for training.\n",
    "SAMPLE_RATE = BATCH_SIZE / SAMPLE_SIZE\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.003 if DP else 0.0001\n",
    "\n",
    "DELTA = 0.001 # Set to be less then the inverse of the training dataset (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "NOISE_MULTIPLIER = 2.0 # The amount of noise sampled and added to the average of the gradients in a batch (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "MAX_GRAD_NORM = 1.2 # The maximum L2 norm of per-sample gradients before they are aggregated by the averaging step (from https://opacus.ai/tutorials/building_image_classifier)\n",
    "\n",
    "# Getting model\n",
    "model = models.Deep2DNet(torch)\n",
    "\n",
    "# Setting device to train on\n",
    "cuda_available = torch.cuda.is_available()\n",
    "if cuda_available:\n",
    "    device = torch.device('cuda:0')\n",
    "    model.cuda(device)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    model.cpu()\n",
    "\n",
    "# Optimizer and Loss Function\n",
    "params = model.parameters()\n",
    "optim = torch.optim.Adam(params=params, lr=LEARNING_RATE) # without DP: 0.0001 // with DP: 0.002-0.003\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Setting up Differential Privacy Engine\n",
    "if DP:\n",
    "    privacy_engine = opacus.privacy_engine.PrivacyEngine(\n",
    "        model.real_module, sample_rate=SAMPLE_RATE,\n",
    "        noise_multiplier=NOISE_MULTIPLIER, max_grad_norm=MAX_GRAD_NORM\n",
    "    )\n",
    "    privacy_engine.attach(optim)\n",
    "else:\n",
    "    privacy_engine = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad528d2-f792-486f-a3e7-c518ab91ff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(batch_size, epochs, model, \n",
    "          torch_ref, optim, loss_function, \n",
    "          train_data, train_labels, test_data, \n",
    "          test_labels, input_shape, device, privacy_engine=None):\n",
    "    \n",
    "    # Variables to track\n",
    "    losses = [] # Training losses per batch per epoch\n",
    "    test_accs = []\n",
    "    test_losses = [] # Test losses per epoch\n",
    "    epsilons = [] \n",
    "    alphas = []\n",
    "    epoch_times = [] # Training times for each epoch\n",
    "    best_acc_loss = (0, 0)\n",
    "    best_model = None\n",
    "    \n",
    "    # Divide dataset into batches (sadly remote DataLoaders aren't yet a thing in pysyft)\n",
    "    length = len(train_data)\n",
    "    \n",
    "    if length % batch_size != 0:\n",
    "        cut_data = train_data[:length - length % batch_size]\n",
    "        cut_labels = train_labels[:length - length % batch_size]\n",
    "        \n",
    "    shape = [-1, batch_size]\n",
    "    shape.extend(input_shape)\n",
    "    \n",
    "    batch_data = cut_data.view(shape)\n",
    "    batch_labels = cut_labels.view(-1, batch_size)\n",
    "    \n",
    "    # Prepare indices for randomization of order for each epoch\n",
    "    indices = np.arange(int(length / batch_size))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        epoch_loss = []\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        print(f'###### Epoch {epoch + 1} ######')\n",
    "        for i in indices:\n",
    "            optim.zero_grad()\n",
    "            \n",
    "            output = model(batch_data[int(i)].to(device))\n",
    "            \n",
    "            loss = loss_function(output, batch_labels[int(i)].to(device))\n",
    "            loss_item = loss.item()\n",
    "            \n",
    "            if model.is_local:\n",
    "                loss_value = loss_item\n",
    "            else:\n",
    "                loss_value = loss_item.get(reason=\"To evaluate training progress\", request_block=True, timeout_secs=5)\n",
    "            print(f'Training Loss: {loss_value}')\n",
    "            epoch_loss.append(loss_value)\n",
    "        \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        \n",
    "        # Checking our privacy budget\n",
    "        if privacy_engine is not None:\n",
    "            epsilon_tuple = privacy_engine.get_privacy_spent(DELTA)\n",
    "            epsilon_ptr = epsilon_tuple[0].resolve_pointer_type()\n",
    "            best_alpha_ptr = epsilon_tuple[1].resolve_pointer_type()\n",
    "\n",
    "            epsilon = epsilon_ptr.get(\n",
    "                reason=\"So we dont go over it\",\n",
    "                request_block=True,\n",
    "                timeout_secs=5\n",
    "            )\n",
    "            best_alpha = best_alpha_ptr.get(\n",
    "                reason=\"So we dont go over it\",\n",
    "                request_block=True,\n",
    "                timeout_secs=5\n",
    "            )\n",
    "            if epsilon is None:\n",
    "                epsilon = float(\"-inf\")\n",
    "            if best_alpha is None:\n",
    "                best_alpha = float(\"-inf\")\n",
    "            print(\n",
    "                f\"(ε = {epsilon:.2f}, δ = {DELTA}) for α = {best_alpha}\"\n",
    "            )\n",
    "            epsilons.append(epsilon)\n",
    "            alphas.append(best_alpha)\n",
    "    \n",
    "        test_acc, test_loss = test(model, loss_function, torch_ref, test_data, test_labels, device)\n",
    "        print(f'Test Accuracy: {test_acc} ---- Test Loss: {test_loss}')\n",
    "        \n",
    "        epoch_end = time.time()\n",
    "        print(f\"Epoch time: {int(epoch_end - epoch_start)} seconds\")\n",
    "        \n",
    "        losses.append(epoch_loss)\n",
    "        epoch_times.append(int(epoch_end - epoch_start))\n",
    "        test_accs.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "    return losses, test_accs, test_losses, epsilons, alphas, epoch_times\n",
    "                   \n",
    "            \n",
    "def test(model, loss_function, torch_ref, data, labels, device):\n",
    "    model.eval()\n",
    "    \n",
    "    data = data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    length = len(data)\n",
    "    \n",
    "    with torch_ref.no_grad():\n",
    "        output = model(data)\n",
    "        test_loss = loss_function(output, labels)\n",
    "        prediction = output.argmax(dim=1)\n",
    "        total = prediction.eq(labels).sum().item()\n",
    "        \n",
    "    acc_ptr = total / length\n",
    "    if model.is_local:\n",
    "        acc = acc_ptr\n",
    "        loss = test_loss.item()\n",
    "    else:\n",
    "        acc = acc_ptr.get(reason=\"To evaluate training progress\", request_block=True, timeout_secs=5)\n",
    "        loss = test_loss.item().get(reason=\"To evaluate training progress\", request_block=True, timeout_secs=5)\n",
    "\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4a2d9d-7504-4950-80e8-0e85412d28ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, test_accs, test_losses, epsilons, alphas, epoch_times = train(BATCH_SIZE, EPOCHS, \n",
    "                                                                      model, torch,\n",
    "                                                                      optim, loss_function, \n",
    "                                                                      train_data, train_labels, \n",
    "                                                                      test_data, test_labels, \n",
    "                                                                      [1, 64, 64], device, privacy_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb5c08c-92cc-4f38-8376-3279c257b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking all interesting variables and results in .csv file\n",
    "if TRACKING:\n",
    "    d = {\n",
    "        'model': MODEL,\n",
    "        'dataset': DATASET,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epochs': EPOCHS,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'train_sample_size': SAMPLE_SIZE,\n",
    "        'test_sample_size': len(test_data_ptr),\n",
    "        'val_sample_size': len(val_data),\n",
    "        'delta': DELTA,\n",
    "        'noise_multiplier': NOISE_MULTIPLIER,\n",
    "        'max_grad_norm': MAX_GRAD_NORM,\n",
    "        'dp_used': DP,\n",
    "        'epsilons': epsilons,\n",
    "        'alphas': alphas,\n",
    "        'train_losses': losses,\n",
    "        'test_accs': test_accs,\n",
    "        'test_losses': test_losses,\n",
    "        'val_acc': 0.0, #val_acc,\n",
    "        'val_loss': 0.0, #val_loss,\n",
    "        'epoch_times': epoch_times\n",
    "    }      \n",
    "    df = pd.read_csv('./Results/1DS.csv')\n",
    "    df = df.append(d, ignore_index=True)\n",
    "    df.to_csv('./Results/1DS.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
