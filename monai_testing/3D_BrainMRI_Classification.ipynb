{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f3c3e2d-be58-4d75-8431-36555121d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import monai\n",
    "from monai.data import ImageDataset\n",
    "from monai.transforms import AddChannel, Compose, RandRotate90, Resize, ScaleIntensity, ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ee9ed-00fb-4dde-a491-99bbc884560a",
   "metadata": {},
   "source": [
    " ---- Data handeling ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d4c9995-9674-4067-a109-ef92de0f400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to true if Data was not downloaded yet. Set to false afterwards!\n",
    "# 4.5 GB / ~ 600 Pictures (3D) / 1 excel file for labels\n",
    "DOWNLOAD = False\n",
    "\n",
    "if DOWNLOAD:\n",
    "    data_url = 'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI-T1.tar'\n",
    "    compressed_file = os.sep.join(['Data', 'IXI-T1.tar'])\n",
    "    data_dir = os.sep.join(['Data', 'IXI-T1'])\n",
    "    \n",
    "    # Data download\n",
    "    monai.apps.download_and_extract(data_url, compressed_file, './Data/IXI-T1')\n",
    "\n",
    "    # Labels document download\n",
    "    labels_url = 'http://biomedic.doc.ic.ac.uk/brain-development/downloads/IXI/IXI.xls'\n",
    "    monai.apps.download_url(labels_url, './Data/IXI.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45af03d5-4add-45c7-9c44-b9deb7831bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = [impath for impath in os.listdir('./Data/IXI-T1')]\n",
    "\n",
    "demographic_info = pd.read_excel('./Data/IXI.xls')\n",
    "\n",
    "# Getting lables TODO: Implement multiple possible labels, not just sex.\n",
    "def make_labeled_data(df, images):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for i in images:\n",
    "        ixi_id = int(i[3:6])\n",
    "        row = df.loc[df['IXI_ID'] == ixi_id]\n",
    "        if not row.empty:\n",
    "            data.append(os.sep.join(['Data', 'IXI-T1', i]))\n",
    "            labels.append(row.iat[0, 1] - 1) # Sex labels are 1/2 but need to be 0/1\n",
    "         \n",
    "    return data, labels\n",
    "\n",
    "\n",
    "data, labels = make_labeled_data(demographic_info, images)\n",
    "\n",
    "# Train - Test split\n",
    "TEST_SIZE = 0.2 # How much percent of the data should be Test Data\n",
    "SAMPLE_SIZE = 0.1 # How much of the whole data should be used (1.0 = 566 pictures, 0.1 = 56 pictures)\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "def train_test_split(data, labels):\n",
    "    size = int(len(data) * SAMPLE_SIZE)\n",
    "    split = int(size * TEST_SIZE)\n",
    "    \n",
    "    test_data = data[:split]\n",
    "    train_data = data[split:size]\n",
    "    \n",
    "    test_labels = labels[:split]\n",
    "    train_labels = labels[split:size]\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "    \n",
    "    \n",
    "train_data, train_labels, test_data, test_labels = train_test_split(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91127bec-192c-4356-ab1d-59dd24e9263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([2, 1, 96, 96, 96]) tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Define transforms\n",
    "train_transforms = Compose([ScaleIntensity(), AddChannel(), Resize((96, 96, 96)), RandRotate90(), ToTensor()])\n",
    "val_transforms = Compose([ScaleIntensity(), AddChannel(), Resize((96, 96, 96)), ToTensor()])\n",
    "\n",
    "# Define image dataset, data loader\n",
    "check_ds = ImageDataset(image_files=data, labels=labels, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=BATCH_SIZE, num_workers=2)\n",
    "im, label = monai.utils.misc.first(check_loader)\n",
    "print(type(im), im.shape, label)\n",
    "\n",
    "# Create a training data loader\n",
    "train_ds = ImageDataset(image_files=train_data, labels=train_labels, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# Create a validation data loader\n",
    "val_ds = ImageDataset(image_files=test_data, labels=test_labels, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0be8a1a-3ed9-4af5-b131-8e23a6c1a4ef",
   "metadata": {},
   "source": [
    " ---- Model Specification ----\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a88f01-a963-41b6-af1c-25fe4ebd01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DenseNet, CrossEntropyLoss and Adam optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = monai.networks.nets.DenseNet(spatial_dims=3, in_channels=1, out_channels=2).to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e785d2-7b24-4997-8c4a-306d84ad6417",
   "metadata": {},
   "source": [
    " ---- Model Training ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b7ef0c-edc1-49e9-a3c6-73aa585f9267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/5\n",
      "1/22, train_loss: 0.7606\n",
      "2/22, train_loss: 0.4488\n",
      "3/22, train_loss: 0.4486\n",
      "4/22, train_loss: 0.4422\n",
      "5/22, train_loss: 1.0372\n",
      "6/22, train_loss: 1.0491\n",
      "7/22, train_loss: 0.7550\n",
      "8/22, train_loss: 0.9360\n",
      "9/22, train_loss: 0.7819\n",
      "10/22, train_loss: 1.0338\n",
      "11/22, train_loss: 0.4390\n",
      "12/22, train_loss: 1.0271\n",
      "13/22, train_loss: 1.0085\n",
      "14/22, train_loss: 0.7754\n",
      "15/22, train_loss: 0.7408\n",
      "16/22, train_loss: 0.7523\n",
      "17/22, train_loss: 1.0160\n",
      "18/22, train_loss: 0.4548\n",
      "19/22, train_loss: 0.7242\n",
      "20/22, train_loss: 0.4618\n",
      "21/22, train_loss: 0.9993\n",
      "22/22, train_loss: 0.6784\n",
      "23/22, train_loss: 0.9695\n",
      "epoch 1 average loss: 0.7713\n",
      "----------\n",
      "epoch 2/5\n",
      "1/22, train_loss: 0.7916\n",
      "2/22, train_loss: 0.6175\n",
      "3/22, train_loss: 0.4732\n",
      "4/22, train_loss: 0.7396\n",
      "5/22, train_loss: 0.4812\n",
      "6/22, train_loss: 0.6898\n",
      "7/22, train_loss: 0.9260\n",
      "8/22, train_loss: 0.4619\n",
      "9/22, train_loss: 0.9864\n",
      "10/22, train_loss: 0.5518\n",
      "11/22, train_loss: 0.6121\n",
      "12/22, train_loss: 0.9588\n",
      "13/22, train_loss: 0.5789\n",
      "14/22, train_loss: 0.6707\n",
      "15/22, train_loss: 0.5685\n",
      "16/22, train_loss: 0.9613\n",
      "17/22, train_loss: 0.5814\n",
      "18/22, train_loss: 0.9354\n",
      "19/22, train_loss: 0.6469\n",
      "20/22, train_loss: 1.0010\n",
      "21/22, train_loss: 0.4601\n",
      "22/22, train_loss: 0.4896\n",
      "23/22, train_loss: 0.9059\n",
      "epoch 2 average loss: 0.6995\n",
      "saved new best metric model\n",
      "current epoch: 2 current accuracy: 0.6364 best accuracy: 0.6364 at epoch 2\n",
      "----------\n",
      "epoch 3/5\n",
      "1/22, train_loss: 0.4336\n",
      "2/22, train_loss: 1.0577\n",
      "3/22, train_loss: 0.9949\n",
      "4/22, train_loss: 0.4578\n",
      "5/22, train_loss: 0.6083\n",
      "6/22, train_loss: 0.5942\n",
      "7/22, train_loss: 0.9858\n",
      "8/22, train_loss: 0.4972\n",
      "9/22, train_loss: 0.9466\n",
      "10/22, train_loss: 0.4720\n",
      "11/22, train_loss: 0.8944\n",
      "12/22, train_loss: 0.5425\n",
      "13/22, train_loss: 0.9195\n",
      "14/22, train_loss: 0.6246\n",
      "15/22, train_loss: 0.4755\n",
      "16/22, train_loss: 0.5689\n",
      "17/22, train_loss: 0.8821\n",
      "18/22, train_loss: 0.4666\n",
      "19/22, train_loss: 0.5240\n",
      "20/22, train_loss: 0.5061\n",
      "21/22, train_loss: 0.4092\n",
      "22/22, train_loss: 0.4987\n",
      "23/22, train_loss: 0.5037\n",
      "epoch 3 average loss: 0.6463\n",
      "----------\n",
      "epoch 4/5\n",
      "1/22, train_loss: 0.8696\n",
      "2/22, train_loss: 0.5282\n",
      "3/22, train_loss: 0.9190\n",
      "4/22, train_loss: 0.5907\n",
      "5/22, train_loss: 0.9068\n",
      "6/22, train_loss: 0.8592\n",
      "7/22, train_loss: 0.5534\n",
      "8/22, train_loss: 0.3915\n",
      "9/22, train_loss: 0.9664\n",
      "10/22, train_loss: 0.4682\n",
      "11/22, train_loss: 0.8619\n",
      "12/22, train_loss: 0.4525\n",
      "13/22, train_loss: 0.4281\n",
      "14/22, train_loss: 0.5087\n",
      "15/22, train_loss: 0.6807\n",
      "16/22, train_loss: 0.5626\n",
      "17/22, train_loss: 0.5663\n",
      "18/22, train_loss: 0.8414\n",
      "19/22, train_loss: 0.8507\n",
      "20/22, train_loss: 0.9098\n",
      "21/22, train_loss: 0.8694\n",
      "22/22, train_loss: 0.5685\n",
      "23/22, train_loss: 0.5510\n",
      "epoch 4 average loss: 0.6828\n",
      "saved new best metric model\n",
      "current epoch: 4 current accuracy: 0.9091 best accuracy: 0.9091 at epoch 4\n",
      "----------\n",
      "epoch 5/5\n",
      "1/22, train_loss: 0.4033\n",
      "2/22, train_loss: 0.8403\n",
      "3/22, train_loss: 0.7970\n",
      "4/22, train_loss: 0.4191\n",
      "5/22, train_loss: 0.8283\n",
      "6/22, train_loss: 0.7698\n",
      "7/22, train_loss: 0.4176\n",
      "8/22, train_loss: 0.8489\n",
      "9/22, train_loss: 0.9058\n",
      "10/22, train_loss: 0.4270\n",
      "11/22, train_loss: 0.4820\n",
      "12/22, train_loss: 0.5712\n",
      "13/22, train_loss: 0.6319\n",
      "14/22, train_loss: 0.4288\n",
      "15/22, train_loss: 0.4887\n",
      "16/22, train_loss: 0.4261\n",
      "17/22, train_loss: 0.4202\n",
      "18/22, train_loss: 0.3884\n",
      "19/22, train_loss: 0.5728\n",
      "20/22, train_loss: 0.8306\n",
      "21/22, train_loss: 0.5766\n",
      "22/22, train_loss: 0.3901\n",
      "23/22, train_loss: 0.5622\n",
      "epoch 5 average loss: 0.5838\n",
      "train completed, best_metric: 0.9091 at epoch: 4\n"
     ]
    }
   ],
   "source": [
    "EPOCHS  = 5\n",
    "\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "metric_values = list()\n",
    "\n",
    "# Iterate through Epochs\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{5}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    # Iterate over the batches\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size\n",
    "        print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "        \n",
    "    epoch_loss /= step\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluate the current model in regular interval\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0.0\n",
    "            metric_count = 0\n",
    "            for val_data in val_loader:\n",
    "                val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "                val_outputs = model(val_images)\n",
    "                value = torch.eq(val_outputs.argmax(dim=1), val_labels)\n",
    "                metric_count += len(value)\n",
    "                num_correct += value.sum().item()\n",
    "            metric = num_correct / metric_count\n",
    "            metric_values.append(metric)\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), \"./Data/best_metric_model_classification3d_array.pth\")\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                \"current epoch: {} current accuracy: {:.4f} best accuracy: {:.4f} at epoch {}\".format(\n",
    "                    epoch + 1, metric, best_metric, best_metric_epoch\n",
    "                )\n",
    "            )\n",
    "print(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
